{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabriele16/abinitio-train/blob/main/colab/abinitio-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpAi8g9XmUU"
      },
      "source": [
        "# Train an E(3)NN Interatomic Potential from ab initio data using NequIP or Allegro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyduv5hWIJYP"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/mir-group/allegro/blob/main/logo.png?raw=true\" width=\"30%\">\n",
        "<img src=\"https://github.com/mir-group/nequip/blob/main/logo.png?raw=true\" width=\"30%\">\n",
        "<center/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2shNisM863Wy"
      },
      "source": [
        "### Open in colab and change the runtime to use the GPU if you have not done so already\n",
        "\n",
        "### Install dependencies, these include pytorch, nequip, allegro and other common packages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HboutJMISv2t",
        "outputId": "b81e2d46-b3aa-4667-ecdc-21bb050838e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! rm -rf abinitio-train\n",
        "! git clone  https://github.com/gabriele16/abinitio-train.git\n",
        "! pip install -r requirements.txt\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "id": "hmmS-5nxkySM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n"
      ],
      "metadata": {
        "id": "egxttm2gUsw7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Pytorch and whether it works correctly on the GPU"
      ],
      "metadata": {
        "id": "MZNRk6K8NO-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uZpOvFtImsy2",
        "outputId": "dec04827-4884-411e-89e0-8c884548c35a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************************\n",
            "torch version:  1.12.1+cu102\n",
            "*****************************\n",
            "cuda is available:  True\n",
            "*****************************\n",
            "cuda version:\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "*****************************\n",
            "check which GPU is being used:\n",
            "Fri May 12 16:55:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "*****************************\n",
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"*****************************\")\n",
        "print(\"torch version: \", torch.__version__)\n",
        "print(\"*****************************\")\n",
        "print(\"cuda is available: \", torch.cuda.is_available())\n",
        "print(\"*****************************\")\n",
        "print(\"cuda version:\")\n",
        "!nvcc --version\n",
        "print(\"*****************************\")\n",
        "print(\"check which GPU is being used:\")\n",
        "!nvidia-smi\n",
        "print(\"*****************************\")\n",
        "!which nvcc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrjXfT6L3S51"
      },
      "source": [
        "### Import PyTorch, numpy etc.\n",
        "### Set-up the directory containing the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J51CA0Bod1Jv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "from ase.io import read, write\n",
        "\n",
        "import warnings\n",
        "import os\n",
        "data_dir = \"/content/abinitio-train/datasets\"\n",
        "\n",
        "np.random.seed(0)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(0)\n",
        "else:\n",
        "  torch.manual_seed(0)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HDmxkn3z8_m"
      },
      "source": [
        "### Workflow:\n",
        "* Train: using a data set, train the neural network\n",
        "* Evaluate: evaluate the error on total energies and forces using unseen data\n",
        "* Deploy: convert the Python-based model into a stand-alone potential file for fast execution\n",
        "* Run: run Molecular Dynamics in CP2K "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aEgq6QYFIn"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KuOIippfVfd"
      },
      "source": [
        "Here, we extract some ab initio data set for bulk water or water in contact with graphene"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd abinitio-train/datasets/ && tar -xzvf wat_bil_gra_aimd.tar.gz\n",
        "! cd abinitio-train/datasets/ && tar -xzvf wat_gra_bil_film.tar.gz\n",
        "! cd abinitio-train/datasets/refined_water_gra && for i in *.tar.gz; do tar -xzvf $i; done"
      ],
      "metadata": {
        "id": "CN4y9qQAljpO",
        "outputId": "f15cd341-2372-4557-c731-614c02213d1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wat_bil_gra_aimd/\n",
            "wat_bil_gra_aimd/wat_pos_frc.extxyz\n",
            "wat_bil_gra_aimd/celldata.dat\n",
            "wat_gra_bil_film/\n",
            "wat_gra_bil_film/wat_pos_frc.extxyz\n",
            "wat_gra_bil_film/last_conf.xyz\n",
            "wat_gra_bil_film/celldata.dat\n",
            "wat_bil_gra_25/\n",
            "wat_bil_gra_25/wat_pos_frc.extxyz\n",
            "wat_bil_gra_25/celldata.dat\n",
            "wat_bil_gra_25/last_conf.xyz\n",
            "wat_bil_gra_30/\n",
            "wat_bil_gra_30/wat_pos_frc.extxyz\n",
            "wat_bil_gra_30/celldata.dat\n",
            "wat_bil_gra_30/last_conf.xyz\n",
            "wat_bil_gra_35/\n",
            "wat_bil_gra_35/wat_pos_frc.extxyz\n",
            "wat_bil_gra_35/celldata.dat\n",
            "wat_bil_gra_35/last_conf.xyz\n",
            "wat_bil_gra_40/\n",
            "wat_bil_gra_40/wat_pos_frc.extxyz\n",
            "wat_bil_gra_40/celldata.dat\n",
            "wat_bil_gra_40/last_conf.xyz\n",
            "wat_bil_gra_45/\n",
            "wat_bil_gra_45/wat_pos_frc.extxyz\n",
            "wat_bil_gra_45/celldata.dat\n",
            "wat_bil_gra_45/last_conf.xyz\n",
            "wat_bil_gra_60/\n",
            "wat_bil_gra_60/wat_pos_frc.extxyz\n",
            "wat_bil_gra_60/celldata.dat\n",
            "wat_bil_gra_60/last_conf.xyz\n",
            "wat_bil_gra_BULK/\n",
            "wat_bil_gra_BULK/wat_pos_frc.extxyz\n",
            "wat_bil_gra_BULK/celldata.dat\n",
            "wat_bil_gra_BULK/last_conf.xyz\n",
            "wat_bil_gra_FILM/\n",
            "wat_bil_gra_FILM/wat_pos_frc.extxyz\n",
            "wat_bil_gra_FILM/celldata.dat\n",
            "wat_bil_gra_FILM/last_conf.xyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd abinitio-train/datasets/ && tar -xzvf wat_film_gra.tar.gz"
      ],
      "metadata": {
        "id": "_8iEFXaFnND-",
        "outputId": "a0a878d3-3694-4d80-aa4a-df48da9166d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wat_film_gra/./\n",
            "wat_film_gra/./wat_pos_frc.extxyz\n",
            "wat_film_gra/./celldata.dat\n",
            "wat_film_gra/./last_conf.xyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Below we explicitly write the input to train the model with NequIP/Allegro"
      ],
      "metadata": {
        "id": "PBPmf2CEgP1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allegro_input = \"\"\"\n",
        "# general\n",
        "root: results/water-gra-film\n",
        "run_name: water-gra-film\n",
        "seed: 42\n",
        "dataset_seed: 42\n",
        "append: true\n",
        "# To use float64 with cp2k, need to implement it, for now use float32\n",
        "default_dtype: float64\n",
        "\n",
        "# -- network --\n",
        "model_builders:\n",
        " - allegro.model.Allegro\n",
        " # the typical model builders from `nequip` can still be used:\n",
        " - PerSpeciesRescale\n",
        " - ForceOutput\n",
        " - RescaleEnergyEtc\n",
        "\n",
        "# cutoffs\n",
        "r_max: 4.0\n",
        "avg_num_neighbors: auto\n",
        "\n",
        "# radial basis\n",
        "# Try to use a small cutoff and a large polynomial cutoff p\n",
        "# use p=48 in the Li3PO4 case in the paper since the cutoff is quite small\n",
        "# see https://github.com/mir-group/allegro/discussions/20\n",
        "BesselBasis_trainable: true \n",
        "### Use normalize_basis: true, see https://github.com/mir-group/allegro/discussions/20\n",
        "normalize_basis: true\n",
        "PolynomialCutoff_p: 16   \n",
        "\n",
        "# symmetry\n",
        "l_max: 2\n",
        "parity: o3_full   \n",
        "\n",
        "# Allegro layers:\n",
        "num_layers: 2\n",
        "env_embed_multiplicity: 8\n",
        "embed_initial_edge: true\n",
        "\n",
        "two_body_latent_mlp_latent_dimensions: [32, 64, 128]\n",
        "two_body_latent_mlp_nonlinearity: silu\n",
        "two_body_latent_mlp_initialization: uniform\n",
        "\n",
        "latent_mlp_latent_dimensions: [128]\n",
        "latent_mlp_nonlinearity: silu\n",
        "latent_mlp_initialization: uniform\n",
        "latent_resnet: true\n",
        "\n",
        "env_embed_mlp_latent_dimensions: []\n",
        "env_embed_mlp_nonlinearity: null\n",
        "env_embed_mlp_initialization: uniform\n",
        "\n",
        "# - end allegro layers -\n",
        "\n",
        "# Final MLP to go from Allegro latent space to edge energies:\n",
        "edge_eng_mlp_latent_dimensions: [32]\n",
        "edge_eng_mlp_nonlinearity: null\n",
        "edge_eng_mlp_initialization: uniform\n",
        "\n",
        "include_keys:\n",
        "  - user_label\n",
        "key_mapping:\n",
        "  user_label: label0\n",
        "\n",
        "# -- data --\n",
        "dataset: ase                                                                   \n",
        "#dataset_file_name: /content/abinitio-train/datasets/wat_gra_bil_film/wat_pos_frc.extxyz                     # path to data set file\n",
        "#dataset_file_name: /content/abinitio-train/datasets/refined_water_gra/wat_bil_gra_FILM/wat_pos_frc.extxyz                     # path to data set file\n",
        "dataset_file_name: /content/abinitio-train/datasets/wat_film_gra/wat_pos_frc.extxyz                     # path to data set file\n",
        "\n",
        "ase_args:\n",
        "  format: extxyz\n",
        "\n",
        "# A mapping of chemical species to type indexes is necessary if the dataset is provided with atomic numbers instead of type indexes.\n",
        "#chemical_symbol_to_type:\n",
        "chemical_symbols:\n",
        "  - C\n",
        "  - H\n",
        "  - O\n",
        "\n",
        "# logging\n",
        "wandb: false\n",
        "#wandb_project: allegro-water-tutorial\n",
        "verbose: info\n",
        "log_batch_freq: 10\n",
        "\n",
        "# training\n",
        "n_train: 500\n",
        "n_val: 50\n",
        "batch_size: 10\n",
        "max_epochs: 100\n",
        "learning_rate: 0.002\n",
        "train_val_split: random\n",
        "shuffle: true\n",
        "metrics_key: validation_loss\n",
        "\n",
        "# use an exponential moving average of the weights\n",
        "use_ema: true\n",
        "ema_decay: 0.99\n",
        "ema_use_num_updates: true\n",
        "\n",
        "# loss function\n",
        "loss_coeffs:\n",
        "  forces: 1.\n",
        "  total_energy:\n",
        "    - 1.\n",
        "    - PerAtomMSELoss\n",
        "\n",
        "# optimizer\n",
        "optimizer_name: Adam\n",
        "optimizer_params:\n",
        "  amsgrad: false\n",
        "  betas: !!python/tuple\n",
        "  - 0.9\n",
        "  - 0.999\n",
        "  eps: 1.0e-08\n",
        "  weight_decay: 0.\n",
        "\n",
        "metrics_components:\n",
        "  - - forces                               # key \n",
        "    - mae                                  # \"rmse\" or \"mae\"\n",
        "  - - forces\n",
        "    - rmse\n",
        "  - - total_energy\n",
        "    - mae    \n",
        "  - - total_energy\n",
        "    - mae\n",
        "    - PerAtom: True                        # if true, energy is normalized by the number of atoms\n",
        "\n",
        "# lr scheduler, drop lr if no improvement for 50 epochs\n",
        "lr_scheduler_name: ReduceLROnPlateau\n",
        "lr_scheduler_patience: 25\n",
        "lr_scheduler_factor: 0.5\n",
        "\n",
        "early_stopping_lower_bounds:\n",
        "  LR: 1.0e-5\n",
        "\n",
        "early_stopping_patiences:\n",
        "  validation_loss: 25\n",
        "\"\"\"\n",
        "\n",
        "with open(\"allegro/configs/allegro-water-gra.yaml\", \"w\") as f:\n",
        "    f.write(allegro_input)"
      ],
      "metadata": {
        "id": "Meo8tyV0W_FS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nequip_input = \"\"\"\n",
        "\n",
        "# IMPORTANT: READ THIS\n",
        "\n",
        "# This is a full yaml file with all nequip options.\n",
        "# It is primarily intented to serve as documentation/reference for all options\n",
        "# For a simpler yaml file containing all necessary features to get you started, we strongly recommend to start with configs/example.yaml\n",
        "\n",
        "# Two folders will be used during the training: 'root'/process and 'root'/'run_name'\n",
        "# run_name contains logfiles and saved models\n",
        "# process contains processed data sets\n",
        "# if 'root'/'run_name' exists, 'root'/'run_name'_'year'-'month'-'day'-'hour'-'min'-'s' will be used instead.\n",
        "root: results/water-gra-film\n",
        "run_name: water-gra-film\n",
        "seed: 123 # model seed\n",
        "dataset_seed: 456 # data set seed\n",
        "append: true # set true if a restarted run should append to the previous log file\n",
        "default_dtype: float64 # type of float to use, e.g. float32 and float64\n",
        "allow_tf32: false # whether to use TensorFloat32 if it is available\n",
        "# device:  cuda                                                                   # which device to use. Default: automatically detected cuda or \"cpu\"\n",
        "\n",
        "# network\n",
        "r_max: 5.0 # cutoff radius in length units, here Angstrom, this is an important hyperparamter to scan\n",
        "num_layers: 3 # number of interaction blocks, we find 3-5 to work best\n",
        "\n",
        "l_max: 1 # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower\n",
        "parity: true # whether to include features with odd mirror parityy; often turning parity off gives equally good results but faster networks, so do consider this\n",
        "num_features: 32 # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower\n",
        "\n",
        "# alternatively, the irreps of the features in various parts of the network can be specified directly:\n",
        "# the following options use e3nn irreps notation\n",
        "# either these four options, or the above three options, should be provided--- they cannot be mixed.\n",
        "# chemical_embedding_irreps_out: 32x0e                                              # irreps for the chemical embedding of species\n",
        "# feature_irreps_hidden: 32x0o + 32x0e + 32x1o + 32x1e                              # irreps used for hidden features, here we go up to lmax=1, with even and odd parities; for more accurate but slower networks, use l=2 or higher, smaller number of features is faster\n",
        "# irreps_edge_sh: 0e + 1o                                                           # irreps of the spherical harmonics used for edges. If a single integer, indicates the full SH up to L_max=that_integer\n",
        "# conv_to_output_hidden_irreps_out: 16x0e                                           # irreps used in hidden layer of output block\n",
        "\n",
        "nonlinearity_type: gate # may be 'gate' or 'norm', 'gate' is recommended\n",
        "resnet:\n",
        "  false # set true to make interaction block a resnet-style update\n",
        "  # the resnet update will only be applied when the input and output irreps of the layer are the same\n",
        "\n",
        "# scalar nonlinearities to use — available options are silu, ssp (shifted softplus), tanh, and abs.\n",
        "# Different nonlinearities are specified for e (even) and o (odd) parity;\n",
        "# note that only tanh and abs are correct for o (odd parity).\n",
        "# silu typically works best for even\n",
        "nonlinearity_scalars:\n",
        "  e: silu\n",
        "  o: tanh\n",
        "\n",
        "nonlinearity_gates:\n",
        "  e: silu\n",
        "  o: tanh\n",
        "\n",
        "# radial network basis\n",
        "num_basis: 8 # number of basis functions used in the radial basis, 8 usually works best\n",
        "BesselBasis_trainable: true # set true to train the bessel weights\n",
        "PolynomialCutoff_p: 48 # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance\n",
        "\n",
        "# radial network\n",
        "invariant_layers: 2 # number of radial layers, usually 1-3 works best, smaller is faster\n",
        "invariant_neurons: 64 # number of hidden neurons in radial function, smaller is faster\n",
        "avg_num_neighbors: auto # number of neighbors to divide by, null => no normalization, auto computes it based on dataset\n",
        "use_sc: true # use self-connection or not, usually gives big improvement\n",
        "\n",
        "# to specify different parameters for each convolutional layer, try examples below\n",
        "# layer1_use_sc: true                                                             # use \"layer{i}_\" prefix to specify parameters for only one of the layer,\n",
        "# priority for different definitions:\n",
        "#   invariant_neurons < InteractionBlock_invariant_neurons < layer{i}_invariant_neurons\n",
        "\n",
        "# data set\n",
        "# there are two options to specify a dataset, npz or ase\n",
        "# npz works with npz files, ase can ready any format that ase.io.read can read\n",
        "# in most cases working with the ase option and an extxyz file is by far the simplest way to do it and we strongly recommend using this\n",
        "# simply provide a single extxyz file that contains the structures together with energies and forces (generated with ase.io.write(atoms, format='extxyz', append=True))\n",
        "\n",
        "include_keys:\n",
        "  - user_label\n",
        "key_mapping:\n",
        "  user_label: label0\n",
        "\n",
        "# alternatively, you can read directly from a VASP OUTCAR file (this will only read that single OUTCAR)\n",
        "# # for VASP OUTCAR, the yaml input should be\n",
        "# dataset: ase\n",
        "# dataset_file_name: OUTCAR\n",
        "# ase_args:\n",
        "#   format: vasp-out\n",
        "# important VASP note: the ase vasp parser stores the potential energy to \"free_energy\" instead of \"energy\".\n",
        "# Here, the key_mapping maps the external name (key) to the NequIP default name (value)\n",
        "# key_mapping:\n",
        "#   free_energy: total_energy\n",
        "\n",
        "# npz example\n",
        "# the keys used need to be stated at least once in key_mapping, npz_fixed_field_keys or include_keys\n",
        "# key_mapping is used to map the key in the npz file to the NequIP default values (see data/_key.py)\n",
        "# all arrays are expected to have the shape of (nframe, natom, ?) except the fixed fields\n",
        "# note that if your data set uses pbc, you need to also pass an array that maps to the nequip \"pbc\" key\n",
        "# dataset: npz                                                                       # type of data set, can be npz or ase\n",
        "# dataset_url: http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip           # url to download the npz. optional\n",
        "# dataset_file_name: ./benchmark_data/toluene_ccsd_t-train.npz                       # path to data set file\n",
        "# key_mapping:\n",
        "#   z: atomic_numbers                                                                # atomic species, integers\n",
        "#   E: total_energy                                                                  # total potential eneriges to train to\n",
        "#   F: forces                                                                        # atomic forces to train to\n",
        "#   R: pos                                                                           # raw atomic positions\n",
        "# npz_fixed_field_keys:                                                              # fields that are repeated across different examples\n",
        "#   - atomic_numbers\n",
        "\n",
        "# A list of chemical species found in the data. The NequIP atom types will be named after the chemical symbols and ordered by atomic number in ascending order.\n",
        "# (In this case, NequIP's internal atom type 0 will be named H and type 1 will be named C.)\n",
        "# Atoms in the input will be assigned NequIP atom types according to their atomic numbers.\n",
        "chemical_symbols:\n",
        "  - C\n",
        "  - H\n",
        "  - O\n",
        "\n",
        "# Alternatively, you may explicitly specify which chemical species in the input will map to NequIP atom type 0, which to atom type 1, and so on.\n",
        "# Other than providing an explicit order for the NequIP atom types, this option behaves the same as `chemical_symbols`\n",
        "#chemical_symbol_to_type:\n",
        "#  C: 0\n",
        "#  H: 1\n",
        "#  O: 2\n",
        "\n",
        "# Alternatively, if the dataset has type indices, you may give the names for the types in order:\n",
        "# (this also sets the number of types)\n",
        "# type_names:\n",
        "#   - my_type\n",
        "#   - atom\n",
        "#   - thing\n",
        "\n",
        "# As an alternative option to npz, you can also pass data ase ASE Atoms-objects\n",
        "# This can often be easier to work with, simply make sure the ASE Atoms object\n",
        "# has a calculator for which atoms.get_potential_energy() and atoms.get_forces() are defined\n",
        "dataset: ase\n",
        "#dataset_file_name: ./abinitio-train/datasets/wat_gra_bil_film/wat_pos_frc.extxyz # need to be a format accepted by ase.io.read\n",
        "#dataset_file_name: /content/abinitio-train/datasets/refined_water_gra/wat_bil_gra_FILM/wat_pos_frc.extxyz                     # path to data set file\n",
        "dataset_file_name: /content/abinitio-train/datasets/wat_film_gra/wat_pos_frc.extxyz                     # path to data set file\n",
        "\n",
        "ase_args: # any arguments needed by ase.io.read\n",
        "  format: extxyz\n",
        "\n",
        "# If you want to use a different dataset for validation, you can specify\n",
        "# the same types of options using a `validation_` prefix:\n",
        "# validation_dataset: ase\n",
        "# validation_dataset_file_name: xxx.xyz                                            # need to be a format accepted by ase.io.read\n",
        "\n",
        "# logging\n",
        "#wandb: true # we recommend using wandb for logging\n",
        "#wandb_project: water-example # project name used in wandb\n",
        "#wandb_watch: false\n",
        "\n",
        "# see https://docs.wandb.ai/ref/python/watch\n",
        "# wandb_watch_kwargs:\n",
        "#   log: all\n",
        "#   log_freq: 1\n",
        "#   log_graph: true\n",
        "\n",
        "verbose: info # the same as python logging, e.g. warning, info, debug, error. case insensitive\n",
        "log_batch_freq: 1 # batch frequency, how often to print training errors withinin the same epoch\n",
        "log_epoch_freq: 1 # epoch frequency, how often to print\n",
        "save_checkpoint_freq: -1 # frequency to save the intermediate checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
        "save_ema_checkpoint_freq: -1 # frequency to save the intermediate ema checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
        "\n",
        "# training\n",
        "n_train: 100 # number of training data\n",
        "n_val: 10 # number of validation data\n",
        "learning_rate: 0.005 # learning rate, we found values between 0.01 and 0.005 to work best - this is often one of the most important hyperparameters to tune\n",
        "batch_size: 1 # batch size, we found it important to keep this small for most applications including forces (1-5); for energy-only training, higher batch sizes work better\n",
        "validation_batch_size: 1 # batch size for evaluating the model during validation. This does not affect the training results, but using the highest value possible (<=n_val) without running out of memory will speed up your training.\n",
        "max_epochs: 200 # stop training after _ number of epochs, we set a very large number here, it won't take this long in practice and we will use early stopping instead\n",
        "train_val_split: random # can be random or sequential. if sequential, first n_train elements are training, next n_val are val, else random, usually random is the right choice\n",
        "shuffle: true # If true, the data loader will shuffle the data, usually a good idea\n",
        "metrics_key: validation_loss # metrics used for scheduling and saving best model. Options: `set`_`quantity`, set can be either \"train\" or \"validation, \"quantity\" can be loss or anything that appears in the validation batch step header, such as f_mae, f_rmse, e_mae, e_rmse\n",
        "use_ema: true # if true, use exponential moving average on weights for val/test, usually helps a lot with training, in particular for energy errors\n",
        "ema_decay: 0.99 # ema weight, typically set to 0.99 or 0.999\n",
        "ema_use_num_updates: true # whether to use number of updates when computing averages\n",
        "report_init_validation: true # if True, report the validation error for just initialized model\n",
        "\n",
        "# early stopping based on metrics values.\n",
        "# LR, wall and any keys printed in the log file can be used.\n",
        "# The key can start with Training or validation. If not defined, the validation value will be used.\n",
        "early_stopping_patiences: # stop early if a metric value stopped decreasing for n epochs\n",
        "  validation_loss: 50\n",
        "\n",
        "early_stopping_delta: # If delta is defined, a decrease smaller than delta will not be considered as a decrease\n",
        "  validation_loss: 0.005\n",
        "\n",
        "early_stopping_cumulative_delta: false # If True, the minimum value recorded will not be updated when the decrease is smaller than delta\n",
        "\n",
        "early_stopping_lower_bounds: # stop early if a metric value is lower than the bound\n",
        "  LR: 1.0e-5\n",
        "\n",
        "early_stopping_upper_bounds: # stop early if a metric value is higher than the bound\n",
        "  cumulative_wall: 1.0e+100\n",
        "\n",
        "# loss function\n",
        "loss_coeffs: # different weights to use in a weighted loss functions\n",
        "  forces: 1 # if using PerAtomMSELoss, a default weight of 1:1 on each should work well\n",
        "  total_energy:\n",
        "    - 1\n",
        "    - PerAtomMSELoss\n",
        "\n",
        "# # default loss function is MSELoss, the name has to be exactly the same as those in torch.nn.\n",
        "# the only supprted targets are forces and total_energy\n",
        "\n",
        "# here are some example of more ways to declare different types of loss functions, depending on your application:\n",
        "# loss_coeffs:\n",
        "#   total_energy: MSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 3.0\n",
        "#   - MSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 1.0\n",
        "#   - PerAtomMSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   forces:\n",
        "#   - 1.0\n",
        "#   - PerSpeciesL1Loss\n",
        "#\n",
        "# loss_coeffs: total_energy\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 3.0\n",
        "#   - L1Loss\n",
        "#   forces: 1.0\n",
        "\n",
        "# output metrics\n",
        "metrics_components:\n",
        "  - - forces # key\n",
        "    - mae # \"rmse\" or \"mae\"\n",
        "  - - forces\n",
        "    - rmse\n",
        "  - - forces\n",
        "    - mae\n",
        "    - PerSpecies: True # if true, per species contribution is counted separately\n",
        "      report_per_component: False # if true, statistics on each component (i.e. fx, fy, fz) will be counted separately\n",
        "  - - forces\n",
        "    - rmse\n",
        "    - PerSpecies: True\n",
        "      report_per_component: False\n",
        "  - - total_energy\n",
        "    - mae\n",
        "  - - total_energy\n",
        "    - mae\n",
        "    - PerAtom: True # if true, energy is normalized by the number of atoms\n",
        "\n",
        "# optimizer, may be any optimizer defined in torch.optim\n",
        "# the name `optimizer_name`is case sensitive\n",
        "optimizer_name: Adam # default optimizer is Adam\n",
        "optimizer_amsgrad: false\n",
        "optimizer_betas: !!python/tuple\n",
        "  - 0.9\n",
        "  - 0.999\n",
        "optimizer_eps: 1.0e-08\n",
        "optimizer_weight_decay: 0\n",
        "\n",
        "# gradient clipping using torch.nn.utils.clip_grad_norm_\n",
        "# see https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\n",
        "# setting to inf or null disables it\n",
        "max_gradient_norm: null\n",
        "\n",
        "# lr scheduler, currently only supports the two options listed below, if you need more please file an issue\n",
        "# first: on-plateau, reduce lr by factory of lr_scheduler_factor if metrics_key hasn't improved for lr_scheduler_patience epoch\n",
        "lr_scheduler_name: ReduceLROnPlateau\n",
        "lr_scheduler_patience: 100\n",
        "lr_scheduler_factor: 0.5\n",
        "\n",
        "# second, cosine annealing with warm restart\n",
        "# lr_scheduler_name: CosineAnnealingWarmRestarts\n",
        "# lr_scheduler_T_0: 10000\n",
        "# lr_scheduler_T_mult: 2\n",
        "# lr_scheduler_eta_min: 0\n",
        "# lr_scheduler_last_epoch: -1\n",
        "\n",
        "# we provide a series of options to shift and scale the data\n",
        "# these are for advanced use and usually the defaults work very well\n",
        "# the default is to scale the energies and forces by scaling them by the force standard deviation and to shift the energy by its mean\n",
        "# in certain cases, it can be useful to have a trainable shift/scale and to also have species-dependent shifts/scales for each atom\n",
        "\n",
        "per_species_rescale_scales_trainable: false\n",
        "# whether the scales are trainable. Defaults to False. Optional\n",
        "per_species_rescale_shifts_trainable: false\n",
        "# whether the shifts are trainable. Defaults to False. Optional\n",
        "per_species_rescale_shifts: dataset_per_atom_total_energy_mean\n",
        "# initial atomic energy shift for each species. default to the mean of per atom energy. Optional\n",
        "# the value can be a constant float value, an array for each species, or a string\n",
        "# string option include:\n",
        "# *  \"dataset_per_atom_total_energy_mean\", which computes the per atom average\n",
        "# *  \"dataset_per_species_total_energy_mean\", which automatically compute the per atom energy mean using a GP model\n",
        "per_species_rescale_scales: dataset_forces_rms\n",
        "# initial atomic energy scale for each species. Optional.\n",
        "# the value can be a constant float value, an array for each species, or a string\n",
        "# string option include:\n",
        "# *  \"dataset_per_atom_total_energy_std\", which computes the per atom energy std\n",
        "# *  \"dataset_per_species_total_energy_std\", which uses the GP model uncertainty\n",
        "# *  \"dataset_per_species_forces_rms\", which compute the force rms for each species\n",
        "# If not provided, defaults to dataset_per_species_force_rms or dataset_per_atom_total_energy_std, depending on whether forces are being trained.\n",
        "# per_species_rescale_kwargs:\n",
        "#   total_energy:\n",
        "#     alpha: 0.1\n",
        "#     max_iteration: 20\n",
        "#     stride: 100\n",
        "# keywords for GP decomposition of per specie energy. Optional. Defaults to 0.1\n",
        "# per_species_rescale_arguments_in_dataset_units: True\n",
        "# if explicit numbers are given for the shifts/scales, this parameter must specify whether the given numbers are unitless shifts/scales or are in the units of the dataset. If ``True``, any global rescalings will correctly be applied to the per-species values.\n",
        "\n",
        "# global energy shift and scale\n",
        "# When \"dataset_total_energy_mean\", the mean energy of the dataset. When None, disables the global shift. When a number, used directly.\n",
        "# Warning: if this value is not None, the model is no longer size extensive\n",
        "global_rescale_shift: null\n",
        "\n",
        "# global energy scale. When \"dataset_force_rms\", the RMS of force components in the dataset. When \"dataset_total_energy_std\", the stdev of energies in the dataset. When null, disables the global scale. When a number, used directly.\n",
        "# If not provided, defaults to either dataset_force_rms or dataset_total_energy_std, depending on whether forces are being trained.\n",
        "global_rescale_scale: dataset_forces_rms\n",
        "\n",
        "# whether the shift of the final global energy rescaling should be trainable\n",
        "global_rescale_shift_trainable: false\n",
        "\n",
        "# whether the scale of the final global energy rescaling should be trainable\n",
        "global_rescale_scale_trainable: false\n",
        "# # full block needed for per specie rescale\n",
        "# global_rescale_shift: null\n",
        "# global_rescale_shift_trainable: false\n",
        "# global_rescale_scale: dataset_forces_rms\n",
        "# global_rescale_scale_trainable: false\n",
        "# per_species_rescale_trainable: true\n",
        "# per_species_rescale_shifts: dataset_per_atom_total_energy_mean\n",
        "# per_species_rescale_scales: dataset_per_atom_total_energy_std\n",
        "\n",
        "# # full block needed for global rescale\n",
        "# global_rescale_shift: dataset_total_energy_mean\n",
        "# global_rescale_shift_trainable: false\n",
        "# global_rescale_scale: dataset_forces_rms\n",
        "# global_rescale_scale_trainable: false\n",
        "# per_species_rescale_trainable: false\n",
        "# per_species_rescale_shifts: null\n",
        "# per_species_rescale_scales: null\n",
        "\n",
        "# Options for e3nn's set_optimization_defaults. A dict:\n",
        "# e3nn_optimization_defaults:\n",
        "#   explicit_backward: True\n",
        "\"\"\"\n",
        "\n",
        "!mkdir nequip_train\n",
        "with open(\"nequip_train/water-gra.yaml\", \"w\") as f:\n",
        "    f.write(nequip_input)"
      ],
      "metadata": {
        "id": "TQYNvq6YO0EH",
        "outputId": "0b14f9c3-13cc-4d5b-8a0b-64f63aa768fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘nequip_train’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with Allegro or NequIP"
      ],
      "metadata": {
        "id": "33biTiWVq7Zb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ukSnt_QD5avu",
        "outputId": "598119b2-ec0d-4bdc-e879-ad1dbe1b73d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(atomic_numbers=[2085480, 1], batch=[2085480], cell=[5793, 3, 3], edge_cell_shift=[57075918, 3], edge_index=[2, 57075918], forces=[2085480, 3], pbc=[5793, 3], pos=[2085480, 3], ptr=[5794], total_energy=[5793, 1])\n",
            "    processed data size: ~2305.06 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(5793)...\n",
            "Replace string dataset_forces_rms to 0.8588102922630664\n",
            "Replace string dataset_per_atom_total_energy_mean to -5.7601544138821765\n",
            "Atomic outputs are scaled by: [H, C, O: 0.858810], shifted by [H, C, O: -5.760154].\n",
            "Replace string dataset_forces_rms to 0.8588102922630664\n",
            "Initially outputs are globally scaled by: 0.8588102922630664, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:276: UserWarning: operator() profile_node %884 : int[] = prim::profile_ivalue(%882)\n",
            " does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:108.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "Equivariance test passed; equivariance errors:\n",
            "   Errors are in real units, where relevant.\n",
            "   Please note that the large scale of the typical\n",
            "   shifts to the (atomic) energy can cause\n",
            "   catastrophic cancellation and give incorrectly\n",
            "   the equivariance error as zero for those fields.\n",
            "   node permutation equivariance of field pos                        -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field total_energy         -> max error=2.274e-13\n",
            "   edge & node permutation invariance for field cell                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field forces                     -> max error=3.873e-14\n",
            "   edge & node permutation invariance for field pbc                  -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_cell_shift            -> max error=0.000e+00\n",
            "   node permutation equivariance of field atom_types                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_attrs                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_features              -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_vectors               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_lengths               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_embedding             -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_attrs                 -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_features              -> max error=4.996e-16\n",
            "   edge permutation equivariance of field edge_energy                -> max error=5.551e-16\n",
            "   node permutation equivariance of field atomic_energy              -> max error=8.882e-16\n",
            "   node permutation equivariance of field batch                      -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=pos                 )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=edge_embedding      )     -> max error=1.001e-13\n",
            "   (parity_k=0, did_translate=False, field=edge_attrs          )     -> max error=7.194e-14\n",
            "   (parity_k=0, did_translate=False, field=edge_features       )     -> max error=5.868e-14\n",
            "   (parity_k=0, did_translate=False, field=edge_energy         )     -> max error=2.379e-14\n",
            "   (parity_k=0, did_translate=False, field=atomic_energy       )     -> max error=1.066e-14\n",
            "   (parity_k=0, did_translate=False, field=total_energy        )     -> max error=2.274e-13\n",
            "   (parity_k=0, did_translate=False, field=forces              )     -> max error=7.849e-14\n",
            "   (parity_k=0, did_translate=True , field=pos                 )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=edge_embedding      )     -> max error=1.148e-13\n",
            "   (parity_k=0, did_translate=True , field=edge_attrs          )     -> max error=8.193e-14\n",
            "   (parity_k=0, did_translate=True , field=edge_features       )     -> max error=8.860e-14\n",
            "   (parity_k=0, did_translate=True , field=edge_energy         )     -> max error=3.719e-14\n",
            "   (parity_k=0, did_translate=True , field=atomic_energy       )     -> max error=1.155e-14\n",
            "   (parity_k=0, did_translate=True , field=total_energy        )     -> max error=1.364e-12\n",
            "   (parity_k=0, did_translate=True , field=forces              )     -> max error=1.018e-13\n",
            "   (parity_k=1, did_translate=False, field=pos                 )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=edge_embedding      )     -> max error=1.083e-13\n",
            "   (parity_k=1, did_translate=False, field=edge_attrs          )     -> max error=6.389e-14\n",
            "   (parity_k=1, did_translate=False, field=edge_features       )     -> max error=9.606e-14\n",
            "   (parity_k=1, did_translate=False, field=edge_energy         )     -> max error=5.362e-14\n",
            "   (parity_k=1, did_translate=False, field=atomic_energy       )     -> max error=1.865e-14\n",
            "   (parity_k=1, did_translate=False, field=total_energy        )     -> max error=1.137e-12\n",
            "   (parity_k=1, did_translate=False, field=forces              )     -> max error=7.172e-14\n",
            "   (parity_k=1, did_translate=True , field=pos                 )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=edge_embedding      )     -> max error=1.211e-13\n",
            "   (parity_k=1, did_translate=True , field=edge_attrs          )     -> max error=3.281e-14\n",
            "   (parity_k=1, did_translate=True , field=edge_features       )     -> max error=7.039e-14\n",
            "   (parity_k=1, did_translate=True , field=edge_energy         )     -> max error=3.225e-14\n",
            "   (parity_k=1, did_translate=True , field=atomic_energy       )     -> max error=8.882e-15\n",
            "   (parity_k=1, did_translate=True , field=total_energy        )     -> max error=6.821e-13\n",
            "   (parity_k=1, did_translate=True , field=forces              )     -> max error=4.519e-14\n",
            "Number of weights: 63848\n",
            "Number of trainable weights: 63848\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0    10         1.64         1.19        0.446        0.708        0.937          206        0.573\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    5.922    0.002         1.23        0.438         1.67        0.717        0.952          205        0.568\n",
            "Wall time: 5.922440278000067\n",
            "! Best model        0    1.667\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10            1            1     0.000826        0.643        0.859         8.83       0.0245\n",
            "      1    20        0.886        0.877      0.00898        0.597        0.804         29.3       0.0813\n",
            "      1    30        0.801        0.795      0.00558        0.566        0.766         23.1       0.0641\n",
            "      1    40        0.705        0.704     0.000476        0.537        0.721         6.65       0.0185\n",
            "      1    50        0.583        0.583     0.000592        0.484        0.656          7.5       0.0208\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10        0.618        0.618     2.38e-05        0.492        0.675         1.22      0.00338\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   76.147    0.002        0.835       0.0229        0.857        0.581        0.785         29.6       0.0823\n",
            "! Validation          1   76.147    0.002        0.628     1.87e-05        0.628        0.499        0.681         1.12       0.0031\n",
            "Wall time: 76.14770595800019\n",
            "! Best model        1    0.628\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10        0.459        0.459     0.000364        0.427        0.582          5.8       0.0161\n",
            "      2    20        0.348        0.348      0.00018        0.369        0.507         4.07       0.0113\n",
            "      2    30         0.26         0.26      4.8e-05        0.322        0.438         1.88      0.00523\n",
            "      2    40        0.179        0.178     2.48e-05        0.276        0.363         1.23      0.00342\n",
            "      2    50        0.139        0.139     2.03e-05        0.247         0.32         1.26      0.00349\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10        0.182        0.182     6.06e-06        0.273        0.366        0.615      0.00171\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2  139.421    0.002        0.319     0.000121        0.319         0.35        0.485         2.69      0.00746\n",
            "! Validation          2  139.421    0.002        0.181     1.34e-05        0.181        0.274        0.365        0.894      0.00248\n",
            "Wall time: 139.42110427800026\n",
            "! Best model        2    0.181\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10         0.11         0.11     4.84e-05        0.222        0.285         1.99      0.00554\n",
            "      3    20       0.0986       0.0986      1.9e-05        0.208         0.27         1.03      0.00286\n",
            "      3    30       0.0856       0.0855      1.7e-05        0.194        0.251         1.08      0.00299\n",
            "      3    40       0.0756       0.0756     1.27e-05        0.184        0.236        0.845      0.00235\n",
            "      3    50       0.0725       0.0725     2.03e-05         0.18        0.231         1.15      0.00319\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10       0.0819       0.0819     4.61e-06         0.19        0.246          0.6      0.00167\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3  202.244    0.002       0.0947     3.44e-05       0.0947        0.204        0.264         1.41      0.00391\n",
            "! Validation          3  202.244    0.002       0.0815      1.2e-05       0.0815         0.19        0.245        0.856      0.00238\n",
            "Wall time: 202.24411483600034\n",
            "! Best model        3    0.081\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10       0.0668       0.0668     9.72e-06        0.173        0.222        0.777      0.00216\n",
            "      4    20       0.0628       0.0627     1.11e-05        0.167        0.215          0.9       0.0025\n",
            "      4    30       0.0598       0.0597     3.09e-05        0.162         0.21         1.48      0.00411\n",
            "      4    40       0.0601       0.0601     1.63e-05         0.16         0.21         1.03      0.00285\n",
            "      4    50       0.0547       0.0547     1.41e-05        0.154        0.201        0.947      0.00263\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10       0.0586       0.0586     1.03e-05        0.161        0.208        0.853      0.00237\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4  265.215    0.002       0.0609     2.28e-05        0.061        0.164        0.212         1.19       0.0033\n",
            "! Validation          4  265.215    0.002       0.0578     1.27e-05       0.0578        0.159        0.206        0.907      0.00252\n",
            "Wall time: 265.2155639980001\n",
            "! Best model        4    0.058\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10       0.0492       0.0492     3.47e-05        0.146        0.191         1.54      0.00428\n",
            "      5    20       0.0448       0.0447     7.26e-05         0.14        0.182         2.36      0.00657\n",
            "      5    30       0.0452       0.0452     4.99e-05        0.139        0.182          1.9      0.00528\n",
            "      5    40       0.0401         0.04     2.08e-05        0.132        0.172          1.2      0.00334\n",
            "      5    50       0.0381       0.0381     6.68e-06        0.129        0.168         0.62      0.00172\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10       0.0448       0.0448     2.36e-05        0.141        0.182         1.37      0.00382\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5  328.144    0.002        0.045     2.87e-05        0.045         0.14        0.182         1.32      0.00367\n",
            "! Validation          5  328.144    0.002       0.0444     2.06e-05       0.0444        0.139        0.181         1.23      0.00343\n",
            "Wall time: 328.1441885190002\n",
            "! Best model        5    0.044\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10       0.0401       0.0401     1.38e-05        0.131        0.172        0.973       0.0027\n",
            "      6    20       0.0364       0.0363     2.95e-05        0.125        0.164         1.49      0.00414\n",
            "      6    30       0.0334       0.0334     9.44e-06        0.122        0.157        0.806      0.00224\n",
            "      6    40        0.032        0.032     9.42e-06        0.118        0.154         0.79       0.0022\n",
            "      6    50       0.0328       0.0328     2.53e-05        0.118        0.156         1.28      0.00356\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10       0.0356       0.0355     3.05e-05        0.126        0.162         1.62      0.00449\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6  391.147    0.002       0.0351     2.56e-05       0.0351        0.123        0.161         1.24      0.00345\n",
            "! Validation          6  391.147    0.002       0.0356     2.55e-05       0.0357        0.124        0.162          1.4      0.00389\n",
            "Wall time: 391.147232455\n",
            "! Best model        6    0.036\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10       0.0317       0.0316     3.73e-05        0.116        0.153         1.71      0.00474\n",
            "      7    20       0.0295       0.0294     7.33e-05        0.112        0.147         2.51      0.00696\n",
            "      7    30       0.0285       0.0285     5.34e-06        0.112        0.145        0.645      0.00179\n",
            "      7    40       0.0275       0.0275     8.42e-05        0.109        0.142         2.46      0.00684\n",
            "      7    50       0.0263       0.0262     9.97e-05        0.106        0.139         2.98      0.00828\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10       0.0293       0.0293     2.48e-05        0.114        0.147         1.46      0.00405\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7  454.117    0.002       0.0287     5.37e-05       0.0288        0.111        0.146         1.93      0.00535\n",
            "! Validation          7  454.117    0.002       0.0297     2.15e-05       0.0297        0.113        0.148         1.29      0.00359\n",
            "Wall time: 454.11785514500025\n",
            "! Best model        7    0.030\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10       0.0249       0.0249     4.89e-06        0.104        0.136        0.566      0.00157\n",
            "      8    20       0.0251       0.0251     7.17e-06        0.104        0.136        0.775      0.00215\n",
            "      8    30       0.0256       0.0256     1.55e-05        0.104        0.137        0.991      0.00275\n",
            "      8    40       0.0219       0.0219     5.56e-06       0.0976        0.127        0.594      0.00165\n",
            "      8    50       0.0238       0.0238     7.59e-06        0.101        0.132         0.68      0.00189\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10       0.0251       0.0251     1.35e-05        0.105        0.136         1.03      0.00286\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8  517.125    0.002       0.0244     2.61e-05       0.0244        0.102        0.134         1.26       0.0035\n",
            "! Validation          8  517.125    0.002       0.0256     1.32e-05       0.0256        0.105        0.137         1.01      0.00281\n",
            "Wall time: 517.125013763\n",
            "! Best model        8    0.026\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10       0.0209       0.0209      1.1e-05       0.0946        0.124        0.812      0.00226\n",
            "      9    20       0.0218       0.0218     8.88e-06       0.0964        0.127        0.799      0.00222\n",
            "      9    30        0.022        0.022     1.74e-05       0.0966        0.127         1.05      0.00292\n",
            "      9    40       0.0218       0.0218      8.5e-06       0.0966        0.127        0.659      0.00183\n",
            "      9    50       0.0207       0.0207     9.36e-06       0.0944        0.124        0.789      0.00219\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10       0.0223       0.0223     7.49e-06       0.0991        0.128        0.708      0.00197\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9  580.087    0.002       0.0217     1.18e-05       0.0217       0.0963        0.126        0.855      0.00237\n",
            "! Validation          9  580.087    0.002       0.0228     8.85e-06       0.0228       0.0989         0.13        0.799      0.00222\n",
            "Wall time: 580.0877082480001\n",
            "! Best model        9    0.023\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10       0.0204       0.0203     4.07e-05       0.0938        0.122         1.75      0.00487\n",
            "     10    20       0.0198       0.0198      1.9e-05       0.0927        0.121          1.1      0.00305\n",
            "     10    30       0.0198       0.0197     2.22e-05       0.0914        0.121         1.29      0.00357\n",
            "     10    40       0.0193       0.0192     8.29e-05       0.0904        0.119         2.74       0.0076\n",
            "     10    50       0.0197       0.0197     6.45e-06       0.0913        0.121        0.559      0.00155\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10       0.0203       0.0203     4.26e-06       0.0945        0.122        0.537      0.00149\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10  643.049    0.002       0.0198     2.11e-05       0.0198        0.092        0.121         1.14      0.00318\n",
            "! Validation         10  643.049    0.002       0.0209     6.53e-06       0.0209       0.0945        0.124        0.658      0.00183\n",
            "Wall time: 643.0498908750001\n",
            "! Best model       10    0.021\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10       0.0191       0.0191     9.15e-06       0.0895        0.119        0.725      0.00201\n",
            "     11    20       0.0201         0.02     2.12e-05        0.092        0.122         1.28      0.00355\n",
            "     11    30       0.0178       0.0178     3.32e-06       0.0875        0.115        0.445      0.00124\n",
            "     11    40       0.0177       0.0177     4.36e-05       0.0872        0.114         1.95       0.0054\n",
            "     11    50       0.0178       0.0176     0.000223       0.0864        0.114         4.57       0.0127\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10       0.0188       0.0188     2.87e-06        0.091        0.118        0.416      0.00115\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11  706.052    0.002       0.0184     3.73e-05       0.0185       0.0887        0.117         1.47      0.00408\n",
            "! Validation         11  706.052    0.002       0.0194     5.67e-06       0.0194       0.0911         0.12        0.579      0.00161\n",
            "Wall time: 706.052285154\n",
            "! Best model       11    0.019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10       0.0179       0.0179     1.14e-05       0.0878        0.115        0.778      0.00216\n",
            "     12    20       0.0177       0.0176     1.46e-05       0.0869        0.114        0.877      0.00244\n",
            "     12    30       0.0181       0.0181     5.34e-05       0.0877        0.115         2.15      0.00598\n",
            "     12    40       0.0173       0.0173     8.69e-06       0.0856        0.113        0.737      0.00205\n",
            "     12    50       0.0158       0.0158     1.29e-05       0.0827        0.108        0.926      0.00257\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10       0.0177       0.0177     2.45e-06       0.0882        0.114        0.392      0.00109\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12  769.041    0.002       0.0175     5.37e-05       0.0175       0.0864        0.113         1.81      0.00503\n",
            "! Validation         12  769.041    0.002       0.0183     5.53e-06       0.0183       0.0884        0.116        0.554      0.00154\n",
            "Wall time: 769.042009452\n",
            "! Best model       12    0.018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10       0.0164       0.0164      6.6e-05       0.0837         0.11         2.41      0.00669\n",
            "     13    20        0.017        0.017     5.75e-05       0.0857        0.112         2.15      0.00596\n",
            "     13    30       0.0167       0.0166     6.03e-05       0.0842        0.111          2.3       0.0064\n",
            "     13    40       0.0158       0.0156     0.000195       0.0823        0.107         4.25       0.0118\n",
            "     13    50       0.0169       0.0168      5.6e-05       0.0845        0.111         2.16      0.00599\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10       0.0168       0.0168     2.35e-06       0.0859        0.111         0.38      0.00106\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13  832.017    0.002       0.0165     4.17e-05       0.0165       0.0839         0.11         1.65      0.00459\n",
            "! Validation         13  832.017    0.002       0.0174     5.54e-06       0.0174       0.0862        0.113        0.549      0.00153\n",
            "Wall time: 832.017815446\n",
            "! Best model       13    0.017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10       0.0159       0.0159     8.24e-05       0.0822        0.108         2.73      0.00759\n",
            "     14    20       0.0164       0.0164     4.13e-05       0.0831         0.11          1.9      0.00527\n",
            "     14    30       0.0169       0.0168     1.51e-05       0.0851        0.111         1.02      0.00284\n",
            "     14    40       0.0153       0.0152     7.93e-06       0.0808        0.106        0.678      0.00188\n",
            "     14    50       0.0152       0.0152     1.57e-05         0.08        0.106         1.13      0.00313\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10        0.016        0.016     2.23e-06       0.0839        0.109        0.366      0.00102\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14  894.910    0.002       0.0158     7.73e-05       0.0159       0.0822        0.108         2.22      0.00617\n",
            "! Validation         14  894.910    0.002       0.0166     5.59e-06       0.0166       0.0842        0.111        0.548      0.00152\n",
            "Wall time: 894.910734562\n",
            "! Best model       14    0.017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10       0.0148       0.0147     1.97e-05       0.0797        0.104         1.13      0.00314\n",
            "     15    20       0.0146       0.0146     1.17e-05       0.0792        0.104        0.845      0.00235\n",
            "     15    30       0.0163       0.0163     1.47e-05       0.0832         0.11            1      0.00279\n",
            "     15    40       0.0154       0.0154     6.23e-05       0.0811        0.106          2.3       0.0064\n",
            "     15    50       0.0146       0.0146     4.12e-05       0.0789        0.104         1.85      0.00513\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10       0.0154       0.0154     2.26e-06       0.0822        0.107        0.367      0.00102\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15  957.817    0.002       0.0152     9.77e-05       0.0153       0.0805        0.106         2.34       0.0065\n",
            "! Validation         15  957.817    0.002       0.0159      5.6e-06       0.0159       0.0825        0.108        0.551      0.00153\n",
            "Wall time: 957.8173833280002\n",
            "! Best model       15    0.016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10       0.0154       0.0152     0.000131       0.0801        0.106         3.45      0.00958\n",
            "     16    20        0.015        0.015     2.13e-05       0.0796        0.105         1.22      0.00338\n",
            "     16    30       0.0151        0.015     8.65e-05       0.0794        0.105          2.7       0.0075\n",
            "     16    40       0.0142       0.0142     1.04e-05       0.0777        0.102         0.72        0.002\n",
            "     16    50       0.0142       0.0142     2.05e-05       0.0782        0.102         1.11       0.0031\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10       0.0148       0.0148     2.14e-06       0.0806        0.105        0.352     0.000977\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16 1020.718    0.002       0.0145     5.13e-05       0.0146       0.0787        0.103         1.72      0.00478\n",
            "! Validation         16 1020.718    0.002       0.0153     5.68e-06       0.0153        0.081        0.106        0.551      0.00153\n",
            "Wall time: 1020.718194996\n",
            "! Best model       16    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10       0.0147       0.0146     8.42e-05       0.0778        0.104         2.82      0.00784\n",
            "     17    20       0.0143       0.0142     2.09e-05        0.078        0.102         1.35      0.00375\n",
            "     17    30       0.0138       0.0137     0.000111       0.0762          0.1         3.13      0.00869\n",
            "     17    40       0.0139       0.0139     1.59e-05       0.0778        0.101        0.995      0.00276\n",
            "     17    50       0.0131       0.0131     1.57e-05        0.075       0.0983         1.03      0.00286\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10       0.0143       0.0143     2.12e-06       0.0792        0.103        0.348     0.000966\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17 1083.626    0.002        0.014     4.87e-05       0.0141       0.0773        0.102         1.77      0.00492\n",
            "! Validation         17 1083.626    0.002       0.0148     5.69e-06       0.0148       0.0796        0.105        0.552      0.00153\n",
            "Wall time: 1083.626711892\n",
            "! Best model       17    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10       0.0145       0.0145     8.05e-05       0.0777        0.103         2.72      0.00755\n",
            "     18    20       0.0137       0.0137     5.12e-06       0.0765          0.1        0.569      0.00158\n",
            "     18    30       0.0137       0.0136     4.65e-05       0.0763          0.1         2.01      0.00558\n",
            "     18    40       0.0139       0.0139      4.1e-05       0.0756        0.101          1.9      0.00527\n",
            "     18    50        0.014       0.0137      0.00032       0.0765        0.101         5.45       0.0151\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10       0.0139       0.0139     2.04e-06       0.0779        0.101        0.335      0.00093\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18 1146.613    0.002       0.0136     6.27e-05       0.0137       0.0761          0.1         1.99      0.00553\n",
            "! Validation         18 1146.613    0.002       0.0143     5.79e-06       0.0144       0.0783        0.103        0.553      0.00154\n",
            "Wall time: 1146.6136543490002\n",
            "! Best model       18    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10       0.0132       0.0131     2.25e-05       0.0749       0.0984         1.19       0.0033\n",
            "     19    20       0.0136       0.0135     0.000104       0.0754       0.0998            3      0.00832\n",
            "     19    30       0.0128       0.0128      8.7e-06       0.0747       0.0973        0.776      0.00216\n",
            "     19    40       0.0131        0.013      1.8e-05       0.0749       0.0981        0.882      0.00245\n",
            "     19    50       0.0127       0.0127     4.01e-05       0.0746       0.0968         1.84      0.00512\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10       0.0134       0.0134     2.06e-06       0.0767       0.0995        0.337     0.000936\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19 1209.554    0.002       0.0132     8.11e-05       0.0133        0.075       0.0986         2.15      0.00598\n",
            "! Validation         19 1209.554    0.002       0.0139     5.76e-06       0.0139       0.0771        0.101        0.555      0.00154\n",
            "Wall time: 1209.5539941449997\n",
            "! Best model       19    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10       0.0127       0.0124     0.000257       0.0733       0.0957         4.87       0.0135\n",
            "     20    20       0.0137       0.0136     1.97e-05        0.076          0.1         1.22      0.00339\n",
            "     20    30       0.0124        0.012     0.000382       0.0719       0.0941         5.99       0.0166\n",
            "     20    40       0.0123       0.0122     7.91e-05        0.073        0.095         2.57      0.00713\n",
            "     20    50        0.012        0.012     8.01e-06       0.0714       0.0942        0.736      0.00204\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10        0.013        0.013     2.01e-06       0.0755        0.098        0.329     0.000914\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20 1272.515    0.002       0.0128      0.00013       0.0129       0.0739       0.0972         2.99      0.00832\n",
            "! Validation         20 1272.515    0.002       0.0135     5.83e-06       0.0135       0.0759       0.0997        0.558      0.00155\n",
            "Wall time: 1272.5156614550006\n",
            "! Best model       20    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10        0.013        0.013     5.52e-05       0.0746       0.0977         2.25      0.00626\n",
            "     21    20       0.0137       0.0135     0.000169       0.0753       0.0998         3.95        0.011\n",
            "     21    30       0.0116       0.0116     3.18e-05        0.071       0.0925         1.63      0.00453\n",
            "     21    40       0.0135       0.0133     0.000166        0.074       0.0991         3.95        0.011\n",
            "     21    50       0.0126       0.0125     1.45e-05       0.0732       0.0962         1.09      0.00303\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10       0.0126       0.0126     2.03e-06       0.0744       0.0965        0.329     0.000914\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21 1335.491    0.002       0.0125     7.64e-05       0.0126        0.073        0.096         2.26      0.00627\n",
            "! Validation         21 1335.491    0.002       0.0131     5.85e-06       0.0131       0.0748       0.0983        0.562      0.00156\n",
            "Wall time: 1335.4915387830006\n",
            "! Best model       21    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10       0.0121        0.012     6.65e-05       0.0719       0.0941         2.41      0.00668\n",
            "     22    20       0.0122       0.0122     2.85e-05       0.0717       0.0948         1.58      0.00438\n",
            "     22    30       0.0128       0.0127     4.62e-05       0.0729       0.0969         1.89      0.00525\n",
            "     22    40       0.0121       0.0121     1.83e-05       0.0716       0.0944         1.09      0.00304\n",
            "     22    50       0.0117       0.0115     0.000196         0.07        0.092         4.25       0.0118\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10       0.0123       0.0123     1.98e-06       0.0733       0.0952        0.322     0.000893\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22 1398.354    0.002       0.0121     6.56e-05       0.0122       0.0718       0.0944         2.13      0.00591\n",
            "! Validation         22 1398.354    0.002       0.0127     5.91e-06       0.0127       0.0737       0.0969        0.564      0.00157\n",
            "Wall time: 1398.3540871269997\n",
            "! Best model       22    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10       0.0119       0.0119     1.32e-05       0.0714       0.0938         0.95      0.00264\n",
            "     23    20        0.012        0.012     1.26e-05       0.0715       0.0939         0.85      0.00236\n",
            "     23    30       0.0124       0.0122     0.000182       0.0717        0.095         4.13       0.0115\n",
            "     23    40       0.0115        0.011     0.000446       0.0689       0.0902          6.5        0.018\n",
            "     23    50       0.0123       0.0123     5.31e-06       0.0721       0.0951        0.629      0.00175\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10        0.012        0.012     1.92e-06       0.0723       0.0939        0.314     0.000873\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23 1461.284    0.002       0.0118     0.000124        0.012        0.071       0.0934         2.83      0.00786\n",
            "! Validation         23 1461.284    0.002       0.0124     5.99e-06       0.0124       0.0728       0.0957        0.567      0.00158\n",
            "Wall time: 1461.2842655220002\n",
            "! Best model       23    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10       0.0117       0.0117     3.68e-05       0.0701       0.0928         1.73      0.00482\n",
            "     24    20       0.0111        0.011     3.34e-05       0.0694       0.0902         1.53      0.00426\n",
            "     24    30       0.0114       0.0113     0.000113       0.0695       0.0914         3.24      0.00901\n",
            "     24    40        0.012       0.0114     0.000616       0.0703       0.0917         7.64       0.0212\n",
            "     24    50       0.0113       0.0111     0.000164        0.069       0.0906         3.83       0.0106\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10       0.0116       0.0116     1.94e-06       0.0714       0.0927        0.313      0.00087\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24 1524.158    0.002       0.0117     0.000122       0.0118       0.0705       0.0928         2.71      0.00752\n",
            "! Validation         24 1524.158    0.002       0.0121     6.01e-06       0.0121       0.0718       0.0945         0.57      0.00158\n",
            "Wall time: 1524.1584304140006\n",
            "! Best model       24    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10       0.0113       0.0113     2.15e-05       0.0692       0.0911         1.26      0.00351\n",
            "     25    20       0.0115       0.0115     5.92e-05       0.0701        0.092         2.26      0.00628\n",
            "     25    30       0.0115       0.0115     2.15e-05       0.0693       0.0921         1.25      0.00346\n",
            "     25    40       0.0105       0.0105     1.01e-05       0.0675       0.0881        0.726      0.00202\n",
            "     25    50       0.0107       0.0106      2.7e-05       0.0675       0.0886         1.38      0.00383\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10       0.0114       0.0114     1.94e-06       0.0705       0.0915        0.311     0.000863\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25 1587.071    0.002       0.0113      5.4e-05       0.0114       0.0694       0.0914         1.86      0.00516\n",
            "! Validation         25 1587.071    0.002       0.0118     6.07e-06       0.0118        0.071       0.0933        0.574      0.00159\n",
            "Wall time: 1587.071013582\n",
            "! Best model       25    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10       0.0107       0.0107     5.23e-05       0.0681       0.0888         2.04      0.00565\n",
            "     26    20       0.0111       0.0109      0.00015       0.0686       0.0897         3.63       0.0101\n",
            "     26    30        0.012        0.012     2.68e-05        0.071       0.0939         1.31      0.00363\n",
            "     26    40       0.0113       0.0112     2.49e-05        0.069       0.0911         1.49      0.00414\n",
            "     26    50       0.0114       0.0114     2.45e-05       0.0691       0.0917          1.3       0.0036\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10       0.0111       0.0111     1.97e-06       0.0697       0.0905        0.309     0.000859\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26 1650.031    0.002       0.0111     5.39e-05       0.0111       0.0686       0.0904          1.8      0.00501\n",
            "! Validation         26 1650.031    0.002       0.0115      6.1e-06       0.0116       0.0701       0.0923        0.576       0.0016\n",
            "Wall time: 1650.0316372440002\n",
            "! Best model       26    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10       0.0106       0.0106     1.63e-06       0.0667       0.0884        0.308     0.000857\n",
            "     27    20       0.0109       0.0109      5.4e-05        0.068       0.0895         2.17      0.00603\n",
            "     27    30       0.0114       0.0113     8.48e-05       0.0693       0.0913         2.77       0.0077\n",
            "     27    40       0.0111        0.011     3.75e-05       0.0684       0.0901         1.73       0.0048\n",
            "     27    50       0.0107       0.0107     2.25e-05       0.0674       0.0888         1.19      0.00331\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10       0.0109       0.0109     1.92e-06       0.0689       0.0895        0.303     0.000842\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27 1712.892    0.002       0.0109     5.34e-05        0.011       0.0681       0.0897         1.88      0.00521\n",
            "! Validation         27 1712.892    0.002       0.0113     6.16e-06       0.0113       0.0694       0.0913        0.578      0.00161\n",
            "Wall time: 1712.8922957189998\n",
            "! Best model       27    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10       0.0107       0.0106     7.64e-05       0.0679       0.0884         2.62      0.00728\n",
            "     28    20       0.0106       0.0106     1.44e-05       0.0663       0.0885          1.1      0.00306\n",
            "     28    30       0.0104       0.0104      3.8e-05       0.0669       0.0875         1.72      0.00477\n",
            "     28    40       0.0104       0.0104      3.8e-05       0.0666       0.0875          1.8      0.00501\n",
            "     28    50       0.0108       0.0105     0.000238       0.0673       0.0881         4.71       0.0131\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10       0.0106       0.0106     1.92e-06       0.0682       0.0885          0.3     0.000834\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28 1775.785    0.002       0.0107     6.63e-05       0.0108       0.0674       0.0888         2.05      0.00569\n",
            "! Validation         28 1775.785    0.002       0.0111     6.21e-06       0.0111       0.0686       0.0903        0.581      0.00161\n",
            "Wall time: 1775.7855782690003\n",
            "! Best model       28    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10      0.00999      0.00998     8.71e-06       0.0659       0.0858        0.781      0.00217\n",
            "     29    20       0.0112       0.0111     6.79e-05       0.0691       0.0905         2.43      0.00674\n",
            "     29    30       0.0103       0.0102     8.44e-05       0.0659       0.0868         2.67      0.00741\n",
            "     29    40       0.0106       0.0106     7.59e-06       0.0669       0.0883        0.712      0.00198\n",
            "     29    50       0.0103       0.0102     1.21e-05       0.0666       0.0869        0.835      0.00232\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10       0.0104       0.0104     1.86e-06       0.0675       0.0876        0.293     0.000813\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29 1838.773    0.002       0.0107     0.000249       0.0109       0.0674       0.0888         3.74       0.0104\n",
            "! Validation         29 1838.773    0.002       0.0108     6.31e-06       0.0109       0.0679       0.0895        0.583      0.00162\n",
            "Wall time: 1838.7730355120002\n",
            "! Best model       29    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10       0.0104       0.0103      5.8e-05       0.0662       0.0872         2.22      0.00618\n",
            "     30    20       0.0111        0.011     0.000104       0.0662       0.0899         3.09      0.00858\n",
            "     30    30      0.00988       0.0098      8.4e-05        0.065        0.085         2.77      0.00769\n",
            "     30    40      0.00995      0.00993     1.83e-05       0.0645       0.0856         1.16      0.00321\n",
            "     30    50      0.00989      0.00987     2.71e-05       0.0648       0.0853         1.43      0.00396\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10       0.0102       0.0102     1.89e-06       0.0668       0.0867        0.292     0.000812\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30 1901.741    0.002       0.0103     6.31e-05       0.0104       0.0661       0.0872         2.01      0.00557\n",
            "! Validation         30 1901.741    0.002       0.0106     6.34e-06       0.0107       0.0673       0.0886        0.585      0.00163\n",
            "Wall time: 1901.7410454689998\n",
            "! Best model       30    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10       0.0101       0.0101      1.7e-05       0.0659       0.0864          1.2      0.00335\n",
            "     31    20       0.0101       0.0101     8.82e-06       0.0653       0.0863        0.787      0.00218\n",
            "     31    30      0.00968      0.00955     0.000136        0.064       0.0839         3.45      0.00959\n",
            "     31    40       0.0104       0.0103     3.83e-05       0.0657       0.0874          1.6      0.00444\n",
            "     31    50       0.0105       0.0105     5.88e-05        0.066       0.0879         2.27      0.00631\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10         0.01         0.01     1.87e-06       0.0662       0.0859        0.287     0.000798\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31 1964.721    0.002       0.0101     4.38e-05       0.0102       0.0655       0.0863         1.51       0.0042\n",
            "! Validation         31 1964.721    0.002       0.0105      6.4e-06       0.0105       0.0667       0.0878        0.586      0.00163\n",
            "Wall time: 1964.7218005700006\n",
            "! Best model       31    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10      0.00949      0.00947     1.87e-05       0.0637       0.0836         1.09      0.00303\n",
            "     32    20      0.00941      0.00939      1.7e-05       0.0633       0.0832         1.19       0.0033\n",
            "     32    30       0.0105       0.0104     0.000145       0.0659       0.0875          3.6         0.01\n",
            "     32    40      0.00943      0.00935     7.24e-05       0.0636       0.0831         2.57      0.00715\n",
            "     32    50       0.0105       0.0105     3.18e-05       0.0661       0.0879          1.6      0.00443\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10      0.00983      0.00983     1.89e-06       0.0656       0.0851        0.288     0.000799\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32 2027.624    0.002      0.00999     0.000133       0.0101       0.0651       0.0859         2.96      0.00821\n",
            "! Validation         32 2027.624    0.002       0.0103     6.42e-06       0.0103       0.0661        0.087        0.588      0.00163\n",
            "Wall time: 2027.624727894\n",
            "! Best model       32    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10       0.0105       0.0103     0.000202       0.0658        0.087         4.33        0.012\n",
            "     33    20       0.0101      0.00985     0.000286       0.0642       0.0852         5.18       0.0144\n",
            "     33    30      0.00994      0.00992     1.95e-05       0.0653       0.0856         1.21      0.00336\n",
            "     33    40       0.0102       0.0102     2.37e-05       0.0638       0.0866         1.39      0.00386\n",
            "     33    50      0.00896      0.00894     1.98e-05       0.0615       0.0812         1.15       0.0032\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10      0.00966      0.00965     1.96e-06        0.065       0.0844          0.3     0.000834\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33 2090.522    0.002      0.00977     0.000132       0.0099       0.0644       0.0849         2.93      0.00815\n",
            "! Validation         33 2090.522    0.002       0.0101     6.42e-06       0.0101       0.0655       0.0863         0.59      0.00164\n",
            "Wall time: 2090.5227529620006\n",
            "! Best model       33    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10      0.00948      0.00947     7.81e-06       0.0635       0.0836        0.661      0.00184\n",
            "     34    20      0.00979      0.00968     0.000113       0.0641       0.0845         3.23      0.00898\n",
            "     34    30       0.0104       0.0103     5.24e-05       0.0653       0.0872          2.1      0.00583\n",
            "     34    40       0.0102      0.00982     0.000391       0.0641       0.0851         6.08       0.0169\n",
            "     34    50       0.0092      0.00918     1.53e-05       0.0623       0.0823        0.893      0.00248\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10      0.00949      0.00949     1.94e-06       0.0644       0.0836        0.294     0.000816\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34 2153.388    0.002      0.00964     0.000133      0.00978       0.0639       0.0843         2.67      0.00743\n",
            "! Validation         34 2153.388    0.002      0.00993     6.48e-06      0.00993       0.0649       0.0856        0.591      0.00164\n",
            "Wall time: 2153.3886145819997\n",
            "! Best model       34    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10      0.00984      0.00973     0.000105       0.0639       0.0847         2.91      0.00809\n",
            "     35    20       0.0102      0.00977     0.000421       0.0641       0.0849         6.29       0.0175\n",
            "     35    30       0.0105       0.0104     3.18e-05       0.0667       0.0878         1.57      0.00435\n",
            "     35    40      0.00982      0.00979     3.27e-05       0.0642        0.085         1.53      0.00424\n",
            "     35    50      0.00982      0.00974     7.74e-05       0.0644       0.0848         2.62      0.00729\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10      0.00933      0.00933     2.03e-06       0.0639       0.0829        0.314     0.000873\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35 2216.238    0.002      0.00964     0.000244      0.00989       0.0639       0.0843         4.05       0.0113\n",
            "! Validation         35 2216.238    0.002      0.00977     6.46e-06      0.00977       0.0644       0.0849        0.594      0.00165\n",
            "Wall time: 2216.2381487230004\n",
            "! Best model       35    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10       0.0093      0.00929     7.06e-06       0.0627       0.0828        0.661      0.00184\n",
            "     36    20      0.00947      0.00937     0.000103       0.0631       0.0831         3.04      0.00843\n",
            "     36    30      0.00966      0.00948     0.000188       0.0635       0.0836         4.15       0.0115\n",
            "     36    40      0.00932      0.00885     0.000473       0.0616       0.0808          6.7       0.0186\n",
            "     36    50         0.01       0.0099     0.000134       0.0644       0.0855         3.53      0.00979\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10      0.00918      0.00918     2.01e-06       0.0633       0.0823        0.308     0.000855\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36 2279.188    0.002      0.00945      0.00013      0.00958       0.0633       0.0835         2.92       0.0081\n",
            "! Validation         36 2279.188    0.002      0.00962      6.5e-06      0.00962       0.0639       0.0842        0.594      0.00165\n",
            "Wall time: 2279.188308336\n",
            "! Best model       36    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10      0.00945      0.00943     1.63e-05       0.0626       0.0834          1.1      0.00307\n",
            "     37    20      0.00838      0.00834     4.27e-05         0.06       0.0784         1.84      0.00511\n",
            "     37    30      0.00915      0.00909     5.43e-05       0.0625       0.0819          2.1      0.00582\n",
            "     37    40      0.00978      0.00977     1.95e-05        0.064       0.0849         1.28      0.00355\n",
            "     37    50      0.00965      0.00886     0.000792       0.0614       0.0808         8.67       0.0241\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10      0.00904      0.00904     2.02e-06       0.0628       0.0816        0.306      0.00085\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37 2342.145    0.002      0.00923     0.000134      0.00937       0.0625       0.0825         2.89      0.00803\n",
            "! Validation         37 2342.145    0.002      0.00947     6.53e-06      0.00948       0.0634       0.0836        0.594      0.00165\n",
            "Wall time: 2342.145340704\n",
            "! Best model       37    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10      0.00914      0.00905     8.99e-05       0.0617       0.0817         2.86      0.00795\n",
            "     38    20      0.00853      0.00852     1.04e-05       0.0606       0.0793        0.839      0.00233\n",
            "     38    30      0.00914      0.00888     0.000264       0.0617       0.0809         4.97       0.0138\n",
            "     38    40      0.00869      0.00867     1.37e-05       0.0609         0.08        0.974      0.00271\n",
            "     38    50      0.00867      0.00866     1.49e-05       0.0612       0.0799            1      0.00279\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10       0.0089       0.0089     2.04e-06       0.0623        0.081        0.311     0.000863\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38 2405.022    0.002      0.00908     9.73e-05      0.00917        0.062       0.0818          2.3       0.0064\n",
            "! Validation         38 2405.022    0.002      0.00934     6.55e-06      0.00934       0.0629        0.083        0.596      0.00166\n",
            "Wall time: 2405.021989334\n",
            "! Best model       38    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10      0.00916      0.00912     4.17e-05       0.0618        0.082         1.92      0.00533\n",
            "     39    20      0.00867      0.00865     1.72e-05       0.0611       0.0799         1.19      0.00331\n",
            "     39    30       0.0089       0.0089     5.46e-06       0.0614        0.081        0.646       0.0018\n",
            "     39    40      0.00926       0.0091     0.000163       0.0619       0.0819         3.89       0.0108\n",
            "     39    50      0.00904      0.00904     6.87e-06       0.0623       0.0816        0.652      0.00181\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10      0.00876      0.00876     2.03e-06       0.0619       0.0804        0.308     0.000855\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39 2467.915    0.002      0.00901     8.98e-05       0.0091       0.0617       0.0815         2.27       0.0063\n",
            "! Validation         39 2467.915    0.002       0.0092      6.6e-06      0.00921       0.0625       0.0824        0.597      0.00166\n",
            "Wall time: 2467.9155666160004\n",
            "! Best model       39    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10      0.00908      0.00902     6.35e-05       0.0623       0.0816         2.37      0.00659\n",
            "     40    20      0.00863      0.00862     1.97e-06       0.0603       0.0797        0.372      0.00103\n",
            "     40    30      0.00856      0.00855     1.11e-05       0.0607       0.0794        0.844      0.00235\n",
            "     40    40      0.00943      0.00865     0.000772       0.0607       0.0799         8.56       0.0238\n",
            "     40    50      0.00934      0.00907     0.000274       0.0617       0.0818         5.06       0.0141\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10      0.00864      0.00864     2.11e-06       0.0614       0.0798        0.321     0.000891\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40 2530.836    0.002      0.00888     0.000127      0.00901       0.0613       0.0809         2.76      0.00766\n",
            "! Validation         40 2530.836    0.002      0.00907     6.58e-06      0.00908        0.062       0.0818          0.6      0.00167\n",
            "Wall time: 2530.8366774120004\n",
            "! Best model       40    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10      0.00891      0.00843     0.000471       0.0594       0.0789         6.69       0.0186\n",
            "     41    20      0.00867      0.00862     4.94e-05         0.06       0.0797         2.06      0.00571\n",
            "     41    30      0.00877      0.00876     6.89e-06       0.0606       0.0804        0.616      0.00171\n",
            "     41    40      0.00913      0.00896     0.000178       0.0615       0.0813         4.04       0.0112\n",
            "     41    50      0.00979      0.00901      0.00078       0.0615       0.0815         8.59       0.0238\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10      0.00852      0.00851     2.12e-06        0.061       0.0792        0.319     0.000886\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41 2593.670    0.002      0.00877     0.000209      0.00898       0.0609       0.0804         3.79       0.0105\n",
            "! Validation         41 2593.670    0.002      0.00895     6.61e-06      0.00896       0.0616       0.0812          0.6      0.00167\n",
            "Wall time: 2593.6707940690003\n",
            "! Best model       41    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10      0.00884      0.00876     7.42e-05       0.0602       0.0804          2.6      0.00721\n",
            "     42    20       0.0082      0.00802     0.000179       0.0582       0.0769         4.09       0.0114\n",
            "     42    30      0.00928      0.00918     0.000104       0.0624       0.0823         3.02      0.00839\n",
            "     42    40      0.00857      0.00851     6.23e-05       0.0605       0.0792         2.29      0.00637\n",
            "     42    50      0.00803      0.00803      7.1e-06       0.0588       0.0769        0.712      0.00198\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10       0.0084       0.0084     2.13e-06       0.0605       0.0787        0.322     0.000894\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42 2656.508    0.002      0.00867     0.000107      0.00878       0.0606         0.08         2.52      0.00699\n",
            "! Validation         42 2656.508    0.002      0.00883     6.63e-06      0.00884       0.0612       0.0807        0.601      0.00167\n",
            "Wall time: 2656.508701017\n",
            "! Best model       42    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10      0.00841       0.0084     9.97e-06       0.0598       0.0787         0.84      0.00233\n",
            "     43    20      0.00843      0.00842     7.92e-06       0.0601       0.0788        0.738      0.00205\n",
            "     43    30      0.00856      0.00851     4.25e-05         0.06       0.0792         1.91      0.00531\n",
            "     43    40      0.00809      0.00798     0.000105       0.0583       0.0767         3.08      0.00856\n",
            "     43    50       0.0093      0.00928     1.41e-05       0.0622       0.0827         1.04      0.00288\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10      0.00828      0.00828     2.08e-06       0.0601       0.0782        0.319     0.000886\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43 2719.223    0.002      0.00848     8.34e-05      0.00856       0.0599       0.0791         2.38       0.0066\n",
            "! Validation         43 2719.223    0.002      0.00872      6.7e-06      0.00872       0.0607       0.0802          0.6      0.00167\n",
            "Wall time: 2719.2237999500003\n",
            "! Best model       43    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10      0.00898      0.00848     0.000499       0.0596       0.0791         6.83        0.019\n",
            "     44    20      0.00879      0.00864     0.000145       0.0604       0.0798         3.68       0.0102\n",
            "     44    30      0.00814      0.00801     0.000127       0.0583       0.0769         3.43      0.00953\n",
            "     44    40      0.00793       0.0079     2.68e-05       0.0585       0.0763         1.51      0.00419\n",
            "     44    50      0.00806      0.00803     2.59e-05       0.0586        0.077         1.49      0.00413\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10      0.00817      0.00817     2.13e-06       0.0597       0.0776        0.324     0.000901\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44 2781.971    0.002      0.00838     0.000204      0.00858       0.0595       0.0786         3.62       0.0101\n",
            "! Validation         44 2781.971    0.002       0.0086      6.7e-06      0.00861       0.0603       0.0797        0.601      0.00167\n",
            "Wall time: 2781.971352677\n",
            "! Best model       44    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10      0.00881      0.00873     8.89e-05       0.0604       0.0802         2.82      0.00783\n",
            "     45    20      0.00801      0.00798     3.62e-05       0.0584       0.0767         1.63      0.00452\n",
            "     45    30      0.00816      0.00794      0.00022       0.0579       0.0765          4.5       0.0125\n",
            "     45    40      0.00798      0.00792     5.53e-05       0.0576       0.0764         2.06      0.00572\n",
            "     45    50      0.00854      0.00835     0.000183       0.0594       0.0785         4.14       0.0115\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10      0.00807      0.00807     2.22e-06       0.0593       0.0771        0.334     0.000929\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45 2844.796    0.002      0.00825     7.85e-05      0.00833        0.059        0.078         2.31      0.00641\n",
            "! Validation         45 2844.796    0.002       0.0085     6.65e-06       0.0085         0.06       0.0792        0.604      0.00168\n",
            "Wall time: 2844.796914673\n",
            "! Best model       45    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10      0.00795      0.00782     0.000122       0.0582        0.076         3.33      0.00926\n",
            "     46    20       0.0081      0.00809     1.25e-05       0.0586       0.0772        0.853      0.00237\n",
            "     46    30      0.00821      0.00815     5.87e-05       0.0592       0.0775         2.13      0.00591\n",
            "     46    40      0.00737      0.00721      0.00016       0.0555       0.0729         3.82       0.0106\n",
            "     46    50       0.0086      0.00835     0.000245       0.0594       0.0785         4.78       0.0133\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10      0.00796      0.00796      2.2e-06       0.0589       0.0766        0.335     0.000929\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46 2907.624    0.002      0.00821     0.000161      0.00838       0.0589       0.0778         3.37      0.00936\n",
            "! Validation         46 2907.624    0.002      0.00839     6.68e-06       0.0084       0.0596       0.0787        0.604      0.00168\n",
            "Wall time: 2907.6247839200005\n",
            "! Best model       46    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10      0.00864      0.00863     8.77e-06       0.0602       0.0798        0.752      0.00209\n",
            "     47    20      0.00787      0.00753      0.00034       0.0567       0.0745         5.69       0.0158\n",
            "     47    30      0.00914      0.00855     0.000597       0.0597       0.0794         7.47       0.0208\n",
            "     47    40      0.00804      0.00802     1.83e-05       0.0582       0.0769         1.04       0.0029\n",
            "     47    50      0.00816       0.0081     5.86e-05        0.059       0.0773         2.21      0.00614\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10      0.00786      0.00786     2.19e-06       0.0585       0.0761        0.335     0.000931\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47 2970.369    0.002      0.00812     0.000124      0.00825       0.0586       0.0774         2.74      0.00762\n",
            "! Validation         47 2970.369    0.002      0.00829     6.71e-06       0.0083       0.0592       0.0782        0.604      0.00168\n",
            "Wall time: 2970.369458475\n",
            "! Best model       47    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10      0.00839      0.00838     1.04e-05       0.0589       0.0786        0.925      0.00257\n",
            "     48    20      0.00816       0.0081     6.08e-05       0.0575       0.0773         2.23       0.0062\n",
            "     48    30      0.00805      0.00799     5.26e-05       0.0584       0.0768         2.17      0.00604\n",
            "     48    40      0.00809      0.00787     0.000218       0.0573       0.0762         4.51       0.0125\n",
            "     48    50      0.00804      0.00774     0.000298       0.0579       0.0756         5.27       0.0146\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10      0.00777      0.00776     2.24e-06       0.0582       0.0757         0.34     0.000946\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48 3033.092    0.002      0.00804     0.000108      0.00815       0.0583        0.077         2.49       0.0069\n",
            "! Validation         48 3033.092    0.002      0.00819      6.7e-06       0.0082       0.0588       0.0777        0.605      0.00168\n",
            "Wall time: 3033.092777862\n",
            "! Best model       48    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10      0.00762      0.00759     2.87e-05       0.0574       0.0748         1.54      0.00428\n",
            "     49    20      0.00867      0.00795     0.000723        0.058       0.0766         8.26        0.023\n",
            "     49    30      0.00933      0.00858     0.000742       0.0601       0.0796         8.37       0.0232\n",
            "     49    40      0.00762       0.0076      2.4e-05       0.0571       0.0749         1.44        0.004\n",
            "     49    50      0.00776      0.00774     2.23e-05       0.0569       0.0756          1.3      0.00362\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10      0.00767      0.00767     2.23e-06       0.0578       0.0752         0.34     0.000945\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49 3095.845    0.002      0.00797     0.000278      0.00825        0.058       0.0767         4.13       0.0115\n",
            "! Validation         49 3095.845    0.002       0.0081     6.73e-06       0.0081       0.0585       0.0773        0.604      0.00168\n",
            "Wall time: 3095.8453583620003\n",
            "! Best model       49    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10      0.00762       0.0076     1.27e-05       0.0569       0.0749        0.828       0.0023\n",
            "     50    20      0.00827      0.00823     3.85e-05       0.0586       0.0779         1.82      0.00506\n",
            "     50    30      0.00769       0.0076     8.71e-05       0.0571       0.0749         2.79      0.00776\n",
            "     50    40      0.00815      0.00781     0.000338       0.0573       0.0759         5.61       0.0156\n",
            "     50    50      0.00877      0.00825     0.000523       0.0592        0.078         7.01       0.0195\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10      0.00759      0.00758     2.26e-06       0.0575       0.0748        0.344     0.000957\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50 3158.635    0.002      0.00789     0.000129      0.00802       0.0577       0.0763         2.83      0.00786\n",
            "! Validation         50 3158.635    0.002      0.00801     6.71e-06      0.00801       0.0581       0.0768        0.605      0.00168\n",
            "Wall time: 3158.6359766690002\n",
            "! Best model       50    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10      0.00846      0.00841     4.33e-05       0.0595       0.0788         1.68      0.00467\n",
            "     51    20      0.00804       0.0078     0.000241       0.0573       0.0758         4.73       0.0131\n",
            "     51    30      0.00803      0.00792     0.000114       0.0576       0.0764         3.26      0.00904\n",
            "     51    40       0.0076      0.00758     1.79e-05       0.0563       0.0748         1.06      0.00294\n",
            "     51    50      0.00805      0.00789     0.000159        0.058       0.0763         3.85       0.0107\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10       0.0075       0.0075     2.33e-06       0.0572       0.0744        0.352     0.000977\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51 3221.377    0.002      0.00791     0.000247      0.00816       0.0578       0.0764         4.08       0.0113\n",
            "! Validation         51 3221.377    0.002      0.00792     6.68e-06      0.00792       0.0578       0.0764        0.608      0.00169\n",
            "Wall time: 3221.3776444820005\n",
            "! Best model       51    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10      0.00835       0.0082     0.000145       0.0586       0.0778         3.66       0.0102\n",
            "     52    20      0.00808      0.00772     0.000362        0.057       0.0755         5.83       0.0162\n",
            "     52    30      0.00775      0.00773      1.9e-05       0.0568       0.0755         1.08      0.00299\n",
            "     52    40      0.00738      0.00734     4.14e-05       0.0556       0.0736         1.69      0.00469\n",
            "     52    50      0.00765      0.00764     6.69e-06       0.0567       0.0751          0.7      0.00194\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10      0.00742      0.00742     2.25e-06       0.0569        0.074        0.345     0.000957\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52 3284.108    0.002      0.00771     0.000111      0.00782        0.057       0.0754         2.66      0.00738\n",
            "! Validation         52 3284.108    0.002      0.00783     6.75e-06      0.00784       0.0575        0.076        0.605      0.00168\n",
            "Wall time: 3284.1083218290005\n",
            "! Best model       52    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10      0.00803       0.0078     0.000228       0.0572       0.0759         4.63       0.0129\n",
            "     53    20       0.0086      0.00818      0.00042       0.0588       0.0777         6.29       0.0175\n",
            "     53    30      0.00744      0.00741      3.3e-05       0.0562       0.0739         1.69      0.00469\n",
            "     53    40      0.00698      0.00695     3.64e-05       0.0538       0.0716         1.78      0.00494\n",
            "     53    50      0.00787      0.00782     4.51e-05       0.0572        0.076         1.98      0.00551\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10      0.00735      0.00734     2.28e-06       0.0566       0.0736        0.347     0.000964\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53 3346.839    0.002       0.0076     0.000106      0.00771       0.0566       0.0749         2.61      0.00726\n",
            "! Validation         53 3346.839    0.002      0.00775     6.75e-06      0.00776       0.0572       0.0756        0.605      0.00168\n",
            "Wall time: 3346.8390828660004\n",
            "! Best model       53    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10      0.00762      0.00727     0.000353       0.0554       0.0732         5.72       0.0159\n",
            "     54    20      0.00753      0.00749     3.34e-05       0.0561       0.0743         1.67      0.00463\n",
            "     54    30      0.00796      0.00793     3.16e-05       0.0578       0.0765         1.56      0.00433\n",
            "     54    40      0.00708      0.00707     1.54e-05       0.0549       0.0722         1.11      0.00309\n",
            "     54    50      0.00744      0.00736     8.27e-05        0.056       0.0737         2.75      0.00763\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10      0.00727      0.00726     2.37e-06       0.0563       0.0732        0.357     0.000993\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54 3409.592    0.002       0.0075     0.000148      0.00765       0.0563       0.0744         3.13       0.0087\n",
            "! Validation         54 3409.592    0.002      0.00767      6.7e-06      0.00768       0.0569       0.0752        0.609      0.00169\n",
            "Wall time: 3409.592411425\n",
            "! Best model       54    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10      0.00715      0.00695     0.000197       0.0545       0.0716         4.28       0.0119\n",
            "     55    20      0.00683       0.0068     2.84e-05        0.054       0.0708          1.5      0.00416\n",
            "     55    30      0.00711      0.00686     0.000247        0.054       0.0711         4.81       0.0134\n",
            "     55    40      0.00719      0.00718     1.19e-05       0.0557       0.0727        0.756       0.0021\n",
            "     55    50      0.00758      0.00734     0.000245       0.0556       0.0736         4.79       0.0133\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10      0.00719      0.00719     2.36e-06        0.056       0.0728        0.357     0.000993\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55 3472.364    0.002      0.00745     9.42e-05      0.00754       0.0561       0.0741          2.5      0.00694\n",
            "! Validation         55 3472.364    0.002      0.00759     6.72e-06       0.0076       0.0566       0.0748        0.609      0.00169\n",
            "Wall time: 3472.3644563340004\n",
            "! Best model       55    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10      0.00722      0.00709     0.000127       0.0551       0.0723         3.41      0.00947\n",
            "     56    20      0.00751      0.00751     6.94e-06       0.0562       0.0744        0.598      0.00166\n",
            "     56    30      0.00735       0.0073     4.87e-05       0.0551       0.0734         2.09       0.0058\n",
            "     56    40      0.00747      0.00739        8e-05       0.0556       0.0738         2.64      0.00734\n",
            "     56    50      0.00731      0.00722     8.93e-05       0.0552        0.073         2.85      0.00791\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10      0.00712      0.00712     2.35e-06       0.0557       0.0725        0.357     0.000991\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56 3535.079    0.002      0.00741     0.000138      0.00755       0.0559       0.0739         3.05      0.00848\n",
            "! Validation         56 3535.079    0.002      0.00752     6.74e-06      0.00752       0.0563       0.0745        0.608      0.00169\n",
            "Wall time: 3535.079319153\n",
            "! Best model       56    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10      0.00757      0.00755     1.83e-05       0.0564       0.0746         1.11      0.00308\n",
            "     57    20      0.00697      0.00696     9.81e-06       0.0542       0.0717        0.842      0.00234\n",
            "     57    30      0.00819      0.00703      0.00115        0.054        0.072         10.5       0.0291\n",
            "     57    40       0.0092      0.00881     0.000389       0.0608       0.0806         6.04       0.0168\n",
            "     57    50      0.00783      0.00761     0.000225       0.0566       0.0749         4.57       0.0127\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10      0.00705      0.00705     2.38e-06       0.0554       0.0721         0.36        0.001\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57 3597.796    0.002      0.00738     0.000268      0.00765       0.0558       0.0738         4.13       0.0115\n",
            "! Validation         57 3597.796    0.002      0.00744     6.73e-06      0.00745        0.056       0.0741        0.608      0.00169\n",
            "Wall time: 3597.7963440210006\n",
            "! Best model       57    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10      0.00721      0.00693      0.00028       0.0545       0.0715          5.1       0.0142\n",
            "     58    20      0.00768      0.00764      3.2e-05       0.0564       0.0751         1.58      0.00439\n",
            "     58    30        0.007      0.00698     1.89e-05       0.0548       0.0717         1.11      0.00308\n",
            "     58    40      0.00744      0.00717     0.000274       0.0547       0.0727         5.08       0.0141\n",
            "     58    50      0.00818      0.00814     4.33e-05       0.0585       0.0775         1.86      0.00516\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10      0.00699      0.00699     2.41e-06       0.0552       0.0718        0.364      0.00101\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58 3660.510    0.002      0.00723     0.000128      0.00736       0.0552        0.073         2.87      0.00798\n",
            "! Validation         58 3660.510    0.002      0.00737     6.72e-06      0.00738       0.0557       0.0737         0.61      0.00169\n",
            "Wall time: 3660.510988768\n",
            "! Best model       58    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10      0.00698      0.00691      7.6e-05       0.0543       0.0714         2.67      0.00742\n",
            "     59    20      0.00761      0.00754     6.66e-05       0.0554       0.0746         2.27      0.00629\n",
            "     59    30      0.00719      0.00717     1.45e-05       0.0554       0.0727        0.897      0.00249\n",
            "     59    40      0.00746      0.00742     3.73e-05       0.0558        0.074         1.82      0.00506\n",
            "     59    50      0.00718      0.00688     0.000304       0.0538       0.0712         5.29       0.0147\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10      0.00693      0.00693     2.33e-06       0.0549       0.0715        0.354     0.000983\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59 3723.231    0.002      0.00718     7.56e-05      0.00725        0.055       0.0728         2.13      0.00593\n",
            "! Validation         59 3723.231    0.002       0.0073     6.82e-06      0.00731       0.0555       0.0734        0.608      0.00169\n",
            "Wall time: 3723.2310198780006\n",
            "! Best model       59    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10      0.00811      0.00808     3.09e-05       0.0575       0.0772         1.61      0.00448\n",
            "     60    20      0.00792      0.00783     9.21e-05       0.0559        0.076         2.89      0.00802\n",
            "     60    30      0.00716      0.00713     2.12e-05       0.0549       0.0725         1.31      0.00364\n",
            "     60    40      0.00768      0.00745     0.000231       0.0556       0.0741         4.58       0.0127\n",
            "     60    50      0.00711      0.00679     0.000311       0.0538       0.0708         5.41        0.015\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10      0.00686      0.00686     2.35e-06       0.0547       0.0711        0.357     0.000992\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60 3785.945    0.002      0.00713     0.000114      0.00724       0.0548       0.0725         2.77       0.0077\n",
            "! Validation         60 3785.945    0.002      0.00723     6.79e-06      0.00724       0.0552        0.073        0.608      0.00169\n",
            "Wall time: 3785.9454616680005\n",
            "! Best model       60    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10      0.00733      0.00723     0.000105       0.0558        0.073         3.04      0.00844\n",
            "     61    20       0.0068       0.0068      5.5e-06       0.0541       0.0708        0.562      0.00156\n",
            "     61    30      0.00652      0.00649     3.28e-05       0.0527       0.0692         1.65      0.00459\n",
            "     61    40      0.00744      0.00742     1.92e-05       0.0558        0.074         1.14      0.00317\n",
            "     61    50      0.00672      0.00671      9.6e-06        0.053       0.0703        0.761      0.00211\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10       0.0068       0.0068     2.42e-06       0.0544       0.0708        0.365      0.00101\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61 3848.659    0.002      0.00697     0.000101      0.00708       0.0543       0.0717         2.56       0.0071\n",
            "! Validation         61 3848.659    0.002      0.00717     6.74e-06      0.00717        0.055       0.0727        0.609      0.00169\n",
            "Wall time: 3848.6593682410003\n",
            "! Best model       61    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10      0.00698      0.00695     3.22e-05        0.054       0.0716         1.63      0.00453\n",
            "     62    20      0.00762      0.00759     2.55e-05       0.0569       0.0748         1.45      0.00403\n",
            "     62    30      0.00782      0.00721     0.000609       0.0555       0.0729         7.59       0.0211\n",
            "     62    40      0.00766      0.00733     0.000335       0.0558       0.0735         5.63       0.0156\n",
            "     62    50      0.00694      0.00659     0.000345       0.0524       0.0697         5.67       0.0157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10      0.00675      0.00674     2.39e-06       0.0542       0.0705        0.362        0.001\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62 3911.340    0.002      0.00713      0.00024      0.00737       0.0548       0.0725          4.2       0.0117\n",
            "! Validation         62 3911.340    0.002       0.0071     6.78e-06      0.00711       0.0547       0.0724        0.608      0.00169\n",
            "Wall time: 3911.3403428990005\n",
            "! Best model       62    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10      0.00671       0.0066     0.000112       0.0531       0.0697         3.05      0.00848\n",
            "     63    20      0.00696      0.00673     0.000231       0.0538       0.0704         4.62       0.0128\n",
            "     63    30      0.00726      0.00717     8.66e-05       0.0547       0.0727         2.75      0.00764\n",
            "     63    40      0.00703      0.00702     1.18e-05       0.0541       0.0719        0.873      0.00242\n",
            "     63    50      0.00668      0.00657     0.000109       0.0526       0.0696         3.17      0.00882\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10      0.00669      0.00669     2.43e-06        0.054       0.0702        0.367      0.00102\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63 3974.068    0.002      0.00693     0.000206      0.00714       0.0541       0.0715         3.69       0.0102\n",
            "! Validation         63 3974.068    0.002      0.00704     6.75e-06      0.00704       0.0545        0.072        0.609      0.00169\n",
            "Wall time: 3974.068037274\n",
            "! Best model       63    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10      0.00689      0.00677     0.000118       0.0536       0.0707         3.32      0.00921\n",
            "     64    20      0.00743       0.0074     3.06e-05       0.0555       0.0739         1.44        0.004\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nequip-train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nequip/scripts/train.py\", line 78, in main\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nequip/train/trainer.py\", line 778, in train\n",
            "    self.epoch_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nequip/train/trainer.py\", line 916, in epoch_step\n",
            "    self.batch_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nequip/train/trainer.py\", line 830, in batch_step\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 396, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 173, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./results\n",
        "\n",
        "Model='Allegro'\n",
        "\n",
        "if Model=='Allegro':\n",
        "  !nequip-train allegro/configs/allegro-water-gra.yaml  --equivariance-test\n",
        "elif Model=='NequIP':\n",
        "  !nequip-train nequip_train/water-gra.yaml --equivariance-test\n",
        "else:\n",
        "  print(\"Model has to be either Allegro or NequIP\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nequip-train --help"
      ],
      "metadata": {
        "id": "RnqHooUBoIjg",
        "outputId": "d028500c-56a8-458d-fc37-6e1ae2bb2f7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: nequip-train\n",
            "       [-h]\n",
            "       [--equivariance-test [EQUIVARIANCE_TEST]]\n",
            "       [--model-debug-mode]\n",
            "       [--grad-anomaly-mode]\n",
            "       [--log LOG]\n",
            "       config\n",
            "\n",
            "Train (or\n",
            "restart\n",
            "training\n",
            "of) a\n",
            "NequIP\n",
            "model.\n",
            "\n",
            "positional arguments:\n",
            "  config\n",
            "    YAML file\n",
            "    configuring\n",
            "    the model,\n",
            "    dataset,\n",
            "    and other\n",
            "    options\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --equivariance-test [EQUIVARIANCE_TEST]\n",
            "    test the\n",
            "    model's equ\n",
            "    ivariance\n",
            "    before\n",
            "    training on\n",
            "    n (default\n",
            "    1) random\n",
            "    frames from\n",
            "    the dataset\n",
            "  --model-debug-mode\n",
            "    enable\n",
            "    model debug\n",
            "    mode, which\n",
            "    can\n",
            "    sometimes\n",
            "    give much\n",
            "    more useful\n",
            "    error\n",
            "    messages at\n",
            "    the cost of\n",
            "    some speed.\n",
            "    Do not use\n",
            "    for\n",
            "    production\n",
            "    training!\n",
            "  --grad-anomaly-mode\n",
            "    enable\n",
            "    PyTorch\n",
            "    autograd\n",
            "    anomaly\n",
            "    mode to\n",
            "    debug NaN\n",
            "    gradients.\n",
            "    Do not use\n",
            "    for\n",
            "    production\n",
            "    training!\n",
            "  --log LOG\n",
            "    log file to\n",
            "    store all\n",
            "    the screen\n",
            "    logging\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the test error\n",
        "##### We get rather small errors in the forces of ~50 meV/A"
      ],
      "metadata": {
        "id": "Wev2u1UyrVCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nequip-evaluate --train-dir results/water-gra-film/water-gra-film --batch-size 10"
      ],
      "metadata": {
        "id": "KlEaWYSgrXtr",
        "outputId": "abf6f510-5f5e-41d0-8616-816c7ff2a231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n",
            "Loading model... \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nequip-evaluate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nequip/scripts/evaluate.py\", line 230, in main\n",
            "    model = model.to(device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 927, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
            "    module._apply(fn)\n",
            "  [Previous line repeated 2 more times]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 925, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJitSZgLYNNF"
      },
      "source": [
        "### Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoeGtlA02KQ"
      },
      "source": [
        "We now convert the model to a potential file. This makes it independent of NequIP and we can load it in CP2K to run MD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Y3NJJgtDIDNc",
        "outputId": "e197e260-c3d5-4564-e86d-38dae23edac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Loading best_model from training session...\n",
            "INFO:root:Compiled & optimized model.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['cp', 'water-gra-film-deploy-alle.pth', 'wat...>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import subprocess\n",
        "\n",
        "# datetime object containing current date and time\n",
        "depl_time = datetime.now().strftime(\"%d%m%Y-%H%M\")\n",
        "\n",
        "if Model == \"Allegro\":\n",
        "  !nequip-deploy build --train-dir results/water-gra-film/water-gra-film water-gra-film-deploy-alle.pth\n",
        "  cmd = [\"cp\", \"water-gra-film-deploy-alle.pth\", \"water-gra-film-deploy-alle-\"+depl_time+\".pth\"] \n",
        "elif Model == \"NequIP\":\n",
        "   !nequip-deploy build --train-dir results/water-gra-film/water-gra-film water-gra-film-deploy-neq.pth \n",
        "   cmd = [\"cp\", \"water-gra-film-deploy-neq.pth\", \"water-gra-film-deploy-neq-\"+depl_time+\".pth\"] \n",
        "\n",
        "subprocess.Popen(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_is_pretrained = False\n",
        "\n",
        "if model_is_pretrained == False:\n",
        "  ! cp *2023*.pth /content/drive/MyDrive/models_and_datasets/models/.\n",
        "else:\n",
        "  ! cp /content/drive/MyDrive/models_and_datasets/models/*.pth ."
      ],
      "metadata": {
        "id": "9uFoAb6AHD8X"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTzdm_qeAU2r"
      },
      "execution_count": 31,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "interpreter": {
      "hash": "c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}