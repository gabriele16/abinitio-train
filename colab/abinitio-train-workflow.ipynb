{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabriele16/abinitio-train/blob/main/colab/abinitio-train-workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpAi8g9XmUU"
      },
      "source": [
        "# Molecular Dynamics simulations with NequIP, Allegro and CP2K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyduv5hWIJYP"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/mir-group/allegro/blob/main/logo.png?raw=true\" width=\"30%\">\n",
        "<img src=\"https://github.com/mir-group/nequip/blob/main/logo.png?raw=true\" width=\"30%\">\n",
        "<center/>\n",
        "\n",
        "<img src=\"https://github.com/gabriele16/cp2k/blob/master/tools/logo/cp2k_logo_500.png?raw=true\" width=\"30%\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2shNisM863Wy"
      },
      "source": [
        "### Open in colab and change the runtime to use the GPU if you have not done so already\n",
        "\n",
        "### Install dependencies, these include pytorch, nequip, allegro and other common packages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! rm -rf abinitio-train\n",
        "! git clone  https://github.com/gabriele16/abinitio-train.git\n",
        "! pip install requirements.txt\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "id": "hmmS-5nxkySM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n"
      ],
      "metadata": {
        "id": "egxttm2gUsw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Pytorch and whether it works correctly on the GPU"
      ],
      "metadata": {
        "id": "MZNRk6K8NO-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZpOvFtImsy2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"*****************************\")\n",
        "print(\"torch version: \", torch.__version__)\n",
        "print(\"*****************************\")\n",
        "print(\"cuda is available: \", torch.cuda.is_available())\n",
        "print(\"*****************************\")\n",
        "print(\"cuda version:\")\n",
        "!nvcc --version\n",
        "print(\"*****************************\")\n",
        "print(\"check which GPU is being used (important only if attempting compilation of CP2K with CUDA):\")\n",
        "print(\"CP2K is optimized to run on the GPU with the folloging architectures: K20X, K40, K80, P100, V100, Mi50, Mi100, Mi250\")\n",
        "!nvidia-smi\n",
        "print(\"*****************************\")\n",
        "print(\"check path where cuda is installed and adjust EXPORT PATH in the cp2k installation if necessary:\")\n",
        "print(\"*****************************\")\n",
        "!which nvcc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the CP2K repository"
      ],
      "metadata": {
        "id": "Ze1I4czAAvUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -rf cp2k\n",
        "! git clone --recursive https://github.com/gabriele16/cp2k.git cp2k"
      ],
      "metadata": {
        "id": "k8dUXd9aU4bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a precompiled cp2k binary and run a test. \n"
      ],
      "metadata": {
        "id": "HzZRBLqBLzfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! gdown https://drive.google.com/uc?id=1hbFxjV657q5C_09gOE8ebxQFIl23b50u\n",
        "! tar -xzvf cp2k_prebuilt_cuda.tar.gz "
      ],
      "metadata": {
        "id": "Yxxo6R7p42ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r cp2k_prebuilt_cuda/exe cp2k/.\n",
        "! cp -r cp2k_prebuilt_cuda/arch/* cp2k/arch/. \n",
        "! mv cp2k/tools/toolchain cp2k/tools/toolchain_not_built\n",
        "! cp -r cp2k_prebuilt_cuda/toolchain cp2k/tools/."
      ],
      "metadata": {
        "id": "JJEZWl8p42ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://download.pytorch.org/libtorch/cu118/libtorch-cxx11-abi-shared-with-deps-2.0.0%2Bcu118.zip\n",
        "! unzip /content/libtorch-cxx11-abi-shared-with-deps-2.0.0+cu118.zip"
      ],
      "metadata": {
        "id": "mHdsjgnk7w7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 777 cp2k/exe/local_cuda/cp2k.ssmp &&  source /content/cp2k/tools/toolchain/install/setup \n",
        "! cd cp2k/tests/Fist/regtest-allegro/  &&  /content/cp2k/exe/local_cuda/cp2k.ssmp -i Allegro_si_MD.inp "
      ],
      "metadata": {
        "id": "FC08sNkR42q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrjXfT6L3S51"
      },
      "source": [
        "### Import PyTorch, numpy etc.\n",
        "### Set-up the directory containing the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J51CA0Bod1Jv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "from ase.io import read, write\n",
        "\n",
        "import warnings\n",
        "import os\n",
        "data_dir = \"/content/abinitio-train/datasets\"\n",
        "\n",
        "np.random.seed(0)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(0)\n",
        "else:\n",
        "  torch.manual_seed(0)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HDmxkn3z8_m"
      },
      "source": [
        "### Workflow:\n",
        "* Train: using a data set, train the neural network, in this case we train on 1000 configurations and perform validation on 100.\n",
        "* Evaluate: evaluate the error on total energies and forces using unseen data, about 2000 configurations.\n",
        "* Deploy: convert the Python-based model into a stand-alone potential file for fast execution\n",
        "* Run: run Molecular Dynamics in CP2K "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aEgq6QYFIn"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KuOIippfVfd"
      },
      "source": [
        "Here, we will train an Allegro potential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd abinitio-train/datasets/ && tar -xzvf wat_bil_gra_aimd.tar.gz\n",
        "! cd abinitio-train/datasets/ && tar -xzvf wat_gra_bil_film.tar.gz"
      ],
      "metadata": {
        "id": "CN4y9qQAljpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Below we explicitly write the input to train the model with NequIP/Allegro"
      ],
      "metadata": {
        "id": "PBPmf2CEgP1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allegro_input = \"\"\"\n",
        "# general\n",
        "root: results/water-bil-gra-film\n",
        "run_name: water-bil-gra-film\n",
        "seed: 42\n",
        "dataset_seed: 42\n",
        "append: true\n",
        "default_dtype: float32\n",
        "\n",
        "# -- network --\n",
        "model_builders:\n",
        " - allegro.model.Allegro\n",
        " # the typical model builders from `nequip` can still be used:\n",
        " - PerSpeciesRescale\n",
        " - ForceOutput\n",
        " - RescaleEnergyEtc\n",
        "\n",
        "# cutoffs\n",
        "r_max: 7.0\n",
        "avg_num_neighbors: auto\n",
        "\n",
        "# radial basis\n",
        "BesselBasis_trainable: true\n",
        "PolynomialCutoff_p: 6   \n",
        "\n",
        "# symmetry\n",
        "l_max: 1\n",
        "parity: o3_full   \n",
        "\n",
        "# Allegro layers:\n",
        "num_layers: 1\n",
        "env_embed_multiplicity: 8\n",
        "embed_initial_edge: true\n",
        "\n",
        "two_body_latent_mlp_latent_dimensions: [32, 64, 128]\n",
        "two_body_latent_mlp_nonlinearity: silu\n",
        "two_body_latent_mlp_initialization: uniform\n",
        "\n",
        "latent_mlp_latent_dimensions: [128]\n",
        "latent_mlp_nonlinearity: silu\n",
        "latent_mlp_initialization: uniform\n",
        "latent_resnet: true\n",
        "\n",
        "env_embed_mlp_latent_dimensions: []\n",
        "env_embed_mlp_nonlinearity: null\n",
        "env_embed_mlp_initialization: uniform\n",
        "\n",
        "# - end allegro layers -\n",
        "\n",
        "# Final MLP to go from Allegro latent space to edge energies:\n",
        "edge_eng_mlp_latent_dimensions: [32]\n",
        "edge_eng_mlp_nonlinearity: null\n",
        "edge_eng_mlp_initialization: uniform\n",
        "\n",
        "include_keys:\n",
        "  - user_label\n",
        "key_mapping:\n",
        "  user_label: label0\n",
        "\n",
        "# -- data --\n",
        "dataset: ase                                                                   \n",
        "dataset_file_name: /content/abinitio-train/datasets/wat_gra_bil_film/wat_pos_frc.extxyz                     # path to data set file\n",
        "ase_args:\n",
        "  format: extxyz\n",
        "\n",
        "# A mapping of chemical species to type indexes is necessary if the dataset is provided with atomic numbers instead of type indexes.\n",
        "#chemical_symbol_to_type:\n",
        "chemical_symbols:\n",
        "  - C\n",
        "  - H\n",
        "  - O\n",
        "\n",
        "# logging\n",
        "wandb: false\n",
        "#wandb_project: allegro-water-tutorial\n",
        "verbose: info\n",
        "log_batch_freq: 10\n",
        "\n",
        "# training\n",
        "n_train: 400\n",
        "n_val: 50\n",
        "batch_size: 1\n",
        "max_epochs: 50\n",
        "learning_rate: 0.002\n",
        "train_val_split: random\n",
        "shuffle: true\n",
        "metrics_key: validation_loss\n",
        "\n",
        "# use an exponential moving average of the weights\n",
        "use_ema: true\n",
        "ema_decay: 0.99\n",
        "ema_use_num_updates: true\n",
        "\n",
        "# loss function\n",
        "loss_coeffs:\n",
        "  forces: 1.\n",
        "  total_energy:\n",
        "    - 1.\n",
        "    - PerAtomMSELoss\n",
        "\n",
        "# optimizer\n",
        "optimizer_name: Adam\n",
        "optimizer_params:\n",
        "  amsgrad: false\n",
        "  betas: !!python/tuple\n",
        "  - 0.9\n",
        "  - 0.999\n",
        "  eps: 1.0e-08\n",
        "  weight_decay: 0.\n",
        "\n",
        "metrics_components:\n",
        "  - - forces                               # key \n",
        "    - mae                                  # \"rmse\" or \"mae\"\n",
        "  - - forces\n",
        "    - rmse\n",
        "  - - total_energy\n",
        "    - mae    \n",
        "  - - total_energy\n",
        "    - mae\n",
        "    - PerAtom: True                        # if true, energy is normalized by the number of atoms\n",
        "\n",
        "# lr scheduler, drop lr if no improvement for 50 epochs\n",
        "lr_scheduler_name: ReduceLROnPlateau\n",
        "lr_scheduler_patience: 50\n",
        "lr_scheduler_factor: 0.5\n",
        "\n",
        "early_stopping_lower_bounds:\n",
        "  LR: 1.0e-5\n",
        "\n",
        "early_stopping_patiences:\n",
        "  validation_loss: 100\n",
        "\"\"\"\n",
        "\n",
        "with open(\"allegro/configs/allegro-water-bil-gra.yaml\", \"w\") as f:\n",
        "    f.write(allegro_input)"
      ],
      "metadata": {
        "id": "Meo8tyV0W_FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nequip_input = \"\"\"\n",
        "\n",
        "# IMPORTANT: READ THIS\n",
        "\n",
        "# This is a full yaml file with all nequip options.\n",
        "# It is primarily intented to serve as documentation/reference for all options\n",
        "# For a simpler yaml file containing all necessary features to get you started, we strongly recommend to start with configs/example.yaml\n",
        "\n",
        "# Two folders will be used during the training: 'root'/process and 'root'/'run_name'\n",
        "# run_name contains logfiles and saved models\n",
        "# process contains processed data sets\n",
        "# if 'root'/'run_name' exists, 'root'/'run_name'_'year'-'month'-'day'-'hour'-'min'-'s' will be used instead.\n",
        "root: results/water-bil-gra-film\n",
        "run_name: water-bil-gra-film\n",
        "seed: 123 # model seed\n",
        "dataset_seed: 456 # data set seed\n",
        "append: true # set true if a restarted run should append to the previous log file\n",
        "default_dtype: float32 # type of float to use, e.g. float32 and float64\n",
        "allow_tf32: false # whether to use TensorFloat32 if it is available\n",
        "# device:  cuda                                                                   # which device to use. Default: automatically detected cuda or \"cpu\"\n",
        "\n",
        "# network\n",
        "r_max: 7.0 # cutoff radius in length units, here Angstrom, this is an important hyperparamter to scan\n",
        "num_layers: 3 # number of interaction blocks, we find 3-5 to work best\n",
        "\n",
        "l_max: 1 # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower\n",
        "parity: true # whether to include features with odd mirror parityy; often turning parity off gives equally good results but faster networks, so do consider this\n",
        "num_features: 32 # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower\n",
        "\n",
        "# alternatively, the irreps of the features in various parts of the network can be specified directly:\n",
        "# the following options use e3nn irreps notation\n",
        "# either these four options, or the above three options, should be provided--- they cannot be mixed.\n",
        "# chemical_embedding_irreps_out: 32x0e                                              # irreps for the chemical embedding of species\n",
        "# feature_irreps_hidden: 32x0o + 32x0e + 32x1o + 32x1e                              # irreps used for hidden features, here we go up to lmax=1, with even and odd parities; for more accurate but slower networks, use l=2 or higher, smaller number of features is faster\n",
        "# irreps_edge_sh: 0e + 1o                                                           # irreps of the spherical harmonics used for edges. If a single integer, indicates the full SH up to L_max=that_integer\n",
        "# conv_to_output_hidden_irreps_out: 16x0e                                           # irreps used in hidden layer of output block\n",
        "\n",
        "nonlinearity_type: gate # may be 'gate' or 'norm', 'gate' is recommended\n",
        "resnet:\n",
        "  false # set true to make interaction block a resnet-style update\n",
        "  # the resnet update will only be applied when the input and output irreps of the layer are the same\n",
        "\n",
        "# scalar nonlinearities to use â€” available options are silu, ssp (shifted softplus), tanh, and abs.\n",
        "# Different nonlinearities are specified for e (even) and o (odd) parity;\n",
        "# note that only tanh and abs are correct for o (odd parity).\n",
        "# silu typically works best for even\n",
        "nonlinearity_scalars:\n",
        "  e: silu\n",
        "  o: tanh\n",
        "\n",
        "nonlinearity_gates:\n",
        "  e: silu\n",
        "  o: tanh\n",
        "\n",
        "# radial network basis\n",
        "num_basis: 8 # number of basis functions used in the radial basis, 8 usually works best\n",
        "BesselBasis_trainable: true # set true to train the bessel weights\n",
        "PolynomialCutoff_p: 6 # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance\n",
        "\n",
        "# radial network\n",
        "invariant_layers: 2 # number of radial layers, usually 1-3 works best, smaller is faster\n",
        "invariant_neurons: 64 # number of hidden neurons in radial function, smaller is faster\n",
        "avg_num_neighbors: auto # number of neighbors to divide by, null => no normalization, auto computes it based on dataset\n",
        "use_sc: true # use self-connection or not, usually gives big improvement\n",
        "\n",
        "# to specify different parameters for each convolutional layer, try examples below\n",
        "# layer1_use_sc: true                                                             # use \"layer{i}_\" prefix to specify parameters for only one of the layer,\n",
        "# priority for different definitions:\n",
        "#   invariant_neurons < InteractionBlock_invariant_neurons < layer{i}_invariant_neurons\n",
        "\n",
        "# data set\n",
        "# there are two options to specify a dataset, npz or ase\n",
        "# npz works with npz files, ase can ready any format that ase.io.read can read\n",
        "# in most cases working with the ase option and an extxyz file is by far the simplest way to do it and we strongly recommend using this\n",
        "# simply provide a single extxyz file that contains the structures together with energies and forces (generated with ase.io.write(atoms, format='extxyz', append=True))\n",
        "\n",
        "include_keys:\n",
        "  - user_label\n",
        "key_mapping:\n",
        "  user_label: label0\n",
        "\n",
        "# alternatively, you can read directly from a VASP OUTCAR file (this will only read that single OUTCAR)\n",
        "# # for VASP OUTCAR, the yaml input should be\n",
        "# dataset: ase\n",
        "# dataset_file_name: OUTCAR\n",
        "# ase_args:\n",
        "#   format: vasp-out\n",
        "# important VASP note: the ase vasp parser stores the potential energy to \"free_energy\" instead of \"energy\".\n",
        "# Here, the key_mapping maps the external name (key) to the NequIP default name (value)\n",
        "# key_mapping:\n",
        "#   free_energy: total_energy\n",
        "\n",
        "# npz example\n",
        "# the keys used need to be stated at least once in key_mapping, npz_fixed_field_keys or include_keys\n",
        "# key_mapping is used to map the key in the npz file to the NequIP default values (see data/_key.py)\n",
        "# all arrays are expected to have the shape of (nframe, natom, ?) except the fixed fields\n",
        "# note that if your data set uses pbc, you need to also pass an array that maps to the nequip \"pbc\" key\n",
        "# dataset: npz                                                                       # type of data set, can be npz or ase\n",
        "# dataset_url: http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip           # url to download the npz. optional\n",
        "# dataset_file_name: ./benchmark_data/toluene_ccsd_t-train.npz                       # path to data set file\n",
        "# key_mapping:\n",
        "#   z: atomic_numbers                                                                # atomic species, integers\n",
        "#   E: total_energy                                                                  # total potential eneriges to train to\n",
        "#   F: forces                                                                        # atomic forces to train to\n",
        "#   R: pos                                                                           # raw atomic positions\n",
        "# npz_fixed_field_keys:                                                              # fields that are repeated across different examples\n",
        "#   - atomic_numbers\n",
        "\n",
        "# A list of chemical species found in the data. The NequIP atom types will be named after the chemical symbols and ordered by atomic number in ascending order.\n",
        "# (In this case, NequIP's internal atom type 0 will be named H and type 1 will be named C.)\n",
        "# Atoms in the input will be assigned NequIP atom types according to their atomic numbers.\n",
        "chemical_symbols:\n",
        "  - C\n",
        "  - H\n",
        "  - O\n",
        "\n",
        "# Alternatively, you may explicitly specify which chemical species in the input will map to NequIP atom type 0, which to atom type 1, and so on.\n",
        "# Other than providing an explicit order for the NequIP atom types, this option behaves the same as `chemical_symbols`\n",
        "#chemical_symbol_to_type:\n",
        "#  C: 0\n",
        "#  H: 1\n",
        "#  O: 2\n",
        "\n",
        "# Alternatively, if the dataset has type indices, you may give the names for the types in order:\n",
        "# (this also sets the number of types)\n",
        "# type_names:\n",
        "#   - my_type\n",
        "#   - atom\n",
        "#   - thing\n",
        "\n",
        "# As an alternative option to npz, you can also pass data ase ASE Atoms-objects\n",
        "# This can often be easier to work with, simply make sure the ASE Atoms object\n",
        "# has a calculator for which atoms.get_potential_energy() and atoms.get_forces() are defined\n",
        "dataset: ase\n",
        "dataset_file_name: ./abinitio-train/datasets/wat_gra_bil_film/wat_pos_frc.extxyz # need to be a format accepted by ase.io.read\n",
        "ase_args: # any arguments needed by ase.io.read\n",
        "  format: extxyz\n",
        "\n",
        "# If you want to use a different dataset for validation, you can specify\n",
        "# the same types of options using a `validation_` prefix:\n",
        "# validation_dataset: ase\n",
        "# validation_dataset_file_name: xxx.xyz                                            # need to be a format accepted by ase.io.read\n",
        "\n",
        "# logging\n",
        "#wandb: true # we recommend using wandb for logging\n",
        "#wandb_project: water-example # project name used in wandb\n",
        "#wandb_watch: false\n",
        "\n",
        "# see https://docs.wandb.ai/ref/python/watch\n",
        "# wandb_watch_kwargs:\n",
        "#   log: all\n",
        "#   log_freq: 1\n",
        "#   log_graph: true\n",
        "\n",
        "verbose: info # the same as python logging, e.g. warning, info, debug, error. case insensitive\n",
        "log_batch_freq: 1 # batch frequency, how often to print training errors withinin the same epoch\n",
        "log_epoch_freq: 1 # epoch frequency, how often to print\n",
        "save_checkpoint_freq: -1 # frequency to save the intermediate checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
        "save_ema_checkpoint_freq: -1 # frequency to save the intermediate ema checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
        "\n",
        "# training\n",
        "n_train: 200 # number of training data\n",
        "n_val: 20 # number of validation data\n",
        "learning_rate: 0.005 # learning rate, we found values between 0.01 and 0.005 to work best - this is often one of the most important hyperparameters to tune\n",
        "batch_size: 1 # batch size, we found it important to keep this small for most applications including forces (1-5); for energy-only training, higher batch sizes work better\n",
        "validation_batch_size: 1 # batch size for evaluating the model during validation. This does not affect the training results, but using the highest value possible (<=n_val) without running out of memory will speed up your training.\n",
        "max_epochs: 100000 # stop training after _ number of epochs, we set a very large number here, it won't take this long in practice and we will use early stopping instead\n",
        "train_val_split: random # can be random or sequential. if sequential, first n_train elements are training, next n_val are val, else random, usually random is the right choice\n",
        "shuffle: true # If true, the data loader will shuffle the data, usually a good idea\n",
        "metrics_key: validation_loss # metrics used for scheduling and saving best model. Options: `set`_`quantity`, set can be either \"train\" or \"validation, \"quantity\" can be loss or anything that appears in the validation batch step header, such as f_mae, f_rmse, e_mae, e_rmse\n",
        "use_ema: true # if true, use exponential moving average on weights for val/test, usually helps a lot with training, in particular for energy errors\n",
        "ema_decay: 0.99 # ema weight, typically set to 0.99 or 0.999\n",
        "ema_use_num_updates: true # whether to use number of updates when computing averages\n",
        "report_init_validation: true # if True, report the validation error for just initialized model\n",
        "\n",
        "# early stopping based on metrics values.\n",
        "# LR, wall and any keys printed in the log file can be used.\n",
        "# The key can start with Training or validation. If not defined, the validation value will be used.\n",
        "early_stopping_patiences: # stop early if a metric value stopped decreasing for n epochs\n",
        "  validation_loss: 50\n",
        "\n",
        "early_stopping_delta: # If delta is defined, a decrease smaller than delta will not be considered as a decrease\n",
        "  validation_loss: 0.005\n",
        "\n",
        "early_stopping_cumulative_delta: false # If True, the minimum value recorded will not be updated when the decrease is smaller than delta\n",
        "\n",
        "early_stopping_lower_bounds: # stop early if a metric value is lower than the bound\n",
        "  LR: 1.0e-5\n",
        "\n",
        "early_stopping_upper_bounds: # stop early if a metric value is higher than the bound\n",
        "  cumulative_wall: 1.0e+100\n",
        "\n",
        "# loss function\n",
        "loss_coeffs: # different weights to use in a weighted loss functions\n",
        "  forces: 1 # if using PerAtomMSELoss, a default weight of 1:1 on each should work well\n",
        "  total_energy:\n",
        "    - 1\n",
        "    - PerAtomMSELoss\n",
        "\n",
        "# # default loss function is MSELoss, the name has to be exactly the same as those in torch.nn.\n",
        "# the only supprted targets are forces and total_energy\n",
        "\n",
        "# here are some example of more ways to declare different types of loss functions, depending on your application:\n",
        "# loss_coeffs:\n",
        "#   total_energy: MSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 3.0\n",
        "#   - MSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 1.0\n",
        "#   - PerAtomMSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   forces:\n",
        "#   - 1.0\n",
        "#   - PerSpeciesL1Loss\n",
        "#\n",
        "# loss_coeffs: total_energy\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 3.0\n",
        "#   - L1Loss\n",
        "#   forces: 1.0\n",
        "\n",
        "# output metrics\n",
        "metrics_components:\n",
        "  - - forces # key\n",
        "    - mae # \"rmse\" or \"mae\"\n",
        "  - - forces\n",
        "    - rmse\n",
        "  - - forces\n",
        "    - mae\n",
        "    - PerSpecies: True # if true, per species contribution is counted separately\n",
        "      report_per_component: False # if true, statistics on each component (i.e. fx, fy, fz) will be counted separately\n",
        "  - - forces\n",
        "    - rmse\n",
        "    - PerSpecies: True\n",
        "      report_per_component: False\n",
        "  - - total_energy\n",
        "    - mae\n",
        "  - - total_energy\n",
        "    - mae\n",
        "    - PerAtom: True # if true, energy is normalized by the number of atoms\n",
        "\n",
        "# optimizer, may be any optimizer defined in torch.optim\n",
        "# the name `optimizer_name`is case sensitive\n",
        "optimizer_name: Adam # default optimizer is Adam\n",
        "optimizer_amsgrad: false\n",
        "optimizer_betas: !!python/tuple\n",
        "  - 0.9\n",
        "  - 0.999\n",
        "optimizer_eps: 1.0e-08\n",
        "optimizer_weight_decay: 0\n",
        "\n",
        "# gradient clipping using torch.nn.utils.clip_grad_norm_\n",
        "# see https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\n",
        "# setting to inf or null disables it\n",
        "max_gradient_norm: null\n",
        "\n",
        "# lr scheduler, currently only supports the two options listed below, if you need more please file an issue\n",
        "# first: on-plateau, reduce lr by factory of lr_scheduler_factor if metrics_key hasn't improved for lr_scheduler_patience epoch\n",
        "lr_scheduler_name: ReduceLROnPlateau\n",
        "lr_scheduler_patience: 100\n",
        "lr_scheduler_factor: 0.5\n",
        "\n",
        "# second, cosine annealing with warm restart\n",
        "# lr_scheduler_name: CosineAnnealingWarmRestarts\n",
        "# lr_scheduler_T_0: 10000\n",
        "# lr_scheduler_T_mult: 2\n",
        "# lr_scheduler_eta_min: 0\n",
        "# lr_scheduler_last_epoch: -1\n",
        "\n",
        "# we provide a series of options to shift and scale the data\n",
        "# these are for advanced use and usually the defaults work very well\n",
        "# the default is to scale the energies and forces by scaling them by the force standard deviation and to shift the energy by its mean\n",
        "# in certain cases, it can be useful to have a trainable shift/scale and to also have species-dependent shifts/scales for each atom\n",
        "\n",
        "per_species_rescale_scales_trainable: false\n",
        "# whether the scales are trainable. Defaults to False. Optional\n",
        "per_species_rescale_shifts_trainable: false\n",
        "# whether the shifts are trainable. Defaults to False. Optional\n",
        "per_species_rescale_shifts: dataset_per_atom_total_energy_mean\n",
        "# initial atomic energy shift for each species. default to the mean of per atom energy. Optional\n",
        "# the value can be a constant float value, an array for each species, or a string\n",
        "# string option include:\n",
        "# *  \"dataset_per_atom_total_energy_mean\", which computes the per atom average\n",
        "# *  \"dataset_per_species_total_energy_mean\", which automatically compute the per atom energy mean using a GP model\n",
        "per_species_rescale_scales: dataset_forces_rms\n",
        "# initial atomic energy scale for each species. Optional.\n",
        "# the value can be a constant float value, an array for each species, or a string\n",
        "# string option include:\n",
        "# *  \"dataset_per_atom_total_energy_std\", which computes the per atom energy std\n",
        "# *  \"dataset_per_species_total_energy_std\", which uses the GP model uncertainty\n",
        "# *  \"dataset_per_species_forces_rms\", which compute the force rms for each species\n",
        "# If not provided, defaults to dataset_per_species_force_rms or dataset_per_atom_total_energy_std, depending on whether forces are being trained.\n",
        "# per_species_rescale_kwargs:\n",
        "#   total_energy:\n",
        "#     alpha: 0.1\n",
        "#     max_iteration: 20\n",
        "#     stride: 100\n",
        "# keywords for GP decomposition of per specie energy. Optional. Defaults to 0.1\n",
        "# per_species_rescale_arguments_in_dataset_units: True\n",
        "# if explicit numbers are given for the shifts/scales, this parameter must specify whether the given numbers are unitless shifts/scales or are in the units of the dataset. If ``True``, any global rescalings will correctly be applied to the per-species values.\n",
        "\n",
        "# global energy shift and scale\n",
        "# When \"dataset_total_energy_mean\", the mean energy of the dataset. When None, disables the global shift. When a number, used directly.\n",
        "# Warning: if this value is not None, the model is no longer size extensive\n",
        "global_rescale_shift: null\n",
        "\n",
        "# global energy scale. When \"dataset_force_rms\", the RMS of force components in the dataset. When \"dataset_total_energy_std\", the stdev of energies in the dataset. When null, disables the global scale. When a number, used directly.\n",
        "# If not provided, defaults to either dataset_force_rms or dataset_total_energy_std, depending on whether forces are being trained.\n",
        "global_rescale_scale: dataset_forces_rms\n",
        "\n",
        "# whether the shift of the final global energy rescaling should be trainable\n",
        "global_rescale_shift_trainable: false\n",
        "\n",
        "# whether the scale of the final global energy rescaling should be trainable\n",
        "global_rescale_scale_trainable: false\n",
        "# # full block needed for per specie rescale\n",
        "# global_rescale_shift: null\n",
        "# global_rescale_shift_trainable: false\n",
        "# global_rescale_scale: dataset_forces_rms\n",
        "# global_rescale_scale_trainable: false\n",
        "# per_species_rescale_trainable: true\n",
        "# per_species_rescale_shifts: dataset_per_atom_total_energy_mean\n",
        "# per_species_rescale_scales: dataset_per_atom_total_energy_std\n",
        "\n",
        "# # full block needed for global rescale\n",
        "# global_rescale_shift: dataset_total_energy_mean\n",
        "# global_rescale_shift_trainable: false\n",
        "# global_rescale_scale: dataset_forces_rms\n",
        "# global_rescale_scale_trainable: false\n",
        "# per_species_rescale_trainable: false\n",
        "# per_species_rescale_shifts: null\n",
        "# per_species_rescale_scales: null\n",
        "\n",
        "# Options for e3nn's set_optimization_defaults. A dict:\n",
        "# e3nn_optimization_defaults:\n",
        "#   explicit_backward: True\n",
        "\"\"\"\n",
        "\n",
        "!mkdir nequip_train\n",
        "with open(\"nequip_train/water-bil-gra.yaml\", \"w\") as f:\n",
        "    f.write(nequip_input)"
      ],
      "metadata": {
        "id": "TQYNvq6YO0EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with Allegro or NequIP for a maximum of 50 epochs"
      ],
      "metadata": {
        "id": "33biTiWVq7Zb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukSnt_QD5avu"
      },
      "outputs": [],
      "source": [
        "!rm -rf ./results\n",
        "\n",
        "Model='NequIP'\n",
        "\n",
        "if Model=='Allegro':\n",
        "  !nequip-train allegro/configs/allegro-water-bil-gra.yaml  \n",
        "elif Model=='NequIP':\n",
        "  !nequip-train nequip_train/water-bil-gra.yaml\n",
        "else:\n",
        "  print(\"Model has to be either Allegro or NequIP\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nequip-train --help"
      ],
      "metadata": {
        "id": "RnqHooUBoIjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the test error\n",
        "##### We get rather small errors in the forces of ~50 meV/A"
      ],
      "metadata": {
        "id": "Wev2u1UyrVCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nequip-evaluate --train-dir results/water-bil-gra-film/water-bil-gra-film --batch-size 1"
      ],
      "metadata": {
        "id": "KlEaWYSgrXtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJitSZgLYNNF"
      },
      "source": [
        "### Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoeGtlA02KQ"
      },
      "source": [
        "We now convert the model to a potential file. This makes it independent of NequIP and we can load it in CP2K to run MD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3NJJgtDIDNc"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import subprocess\n",
        "\n",
        "# datetime object containing current date and time\n",
        "depl_time = datetime.now().strftime(\"%d%m%Y-%H%M\")\n",
        "\n",
        "if Model == \"Allegro\":\n",
        "  !nequip-deploy build --train-dir results/water-bil-gra-film/water-bil-gra-film water-bil-gra-film-deploy-alle.pth\n",
        "  cmd = [\"cp\", \"water-bil-gra-film-deploy-alle.pth\", \"water-bil-gra-film-deploy-alle-\"+depl_time+\".pth\"] \n",
        "elif Model == \"NequIP\":\n",
        "   !nequip-deploy build --train-dir results/water-bil-gra-film/water-bil-gra-film water-bil-gra-film-deploy-neq.pth \n",
        "   cmd = [\"cp\", \"water-bil-gra-film-deploy-neq.pth\", \"water-bil-gra-film-deploy-neq-\"+depl_time+\".pth\"] \n",
        "\n",
        "subprocess.Popen(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DZSs21VGB_lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_is_pretrained = False\n",
        "\n",
        "if model_is_pretrained == False:\n",
        "  ! cp *2023*.pth /content/drive/MyDrive/models_and_datasets/models/.\n",
        "else:\n",
        "  ! cp /content/drive/MyDrive/models_and_datasets/models/*.pth ."
      ],
      "metadata": {
        "id": "9uFoAb6AHD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4r5FBXaum9n"
      },
      "source": [
        "# CP2K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIYIYyr1B4O"
      },
      "source": [
        "We are now in a position to run MD with our potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQs0ijPhvAGb"
      },
      "source": [
        "Set up a simple cp2k input file, you can find an example file inside ```cp2k/tests/Fist/regtest-allegro```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CAUTION: Be careful with the units of the .pth model file. One can explicitly specify the units of this file since CP2K input defaults as Angstrom for lengths and A.U. for everything else."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AXv_y_JD1TJ"
      },
      "source": [
        "### We now run MD of bulk water at 360 K using 64 molecules in the NVT ensemble for 10 ps (20,000 steps)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ase.build import sort\n",
        "from ase.io import read, write\n",
        "\n",
        "last_conf = sort(read('abinitio-train/datasets/wat_gra_bil_film/wat_pos_frc.extxyz',index=-1))\n",
        "write('last_conf.xyz',last_conf)\n",
        "! head abinitio-train/datasets/wat_bil_gra_aimd/celldata.dat\n",
        "! cat last_conf.xyz"
      ],
      "metadata": {
        "id": "_GhAPAQHHyg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp2k_input_md = \"\"\"\n",
        "&GLOBAL\n",
        "  PROJECT water_bil_gra\n",
        "  RUN_TYPE MD\n",
        "&END GLOBAL\n",
        "&FORCE_EVAL\n",
        "  METHOD FIST\n",
        "  &MM\n",
        "    &FORCEFIELD\n",
        "     &NONBONDED\n",
        "     &ALLEGRO\n",
        "        ATOMS C H O\n",
        "        PARM_FILE_NAME ./water-bil-gra-film-deploy-allegro.pth\n",
        "        UNIT_COORDS angstrom\n",
        "        UNIT_ENERGY eV\n",
        "        UNIT_FORCES eV*Angstrom^-1\n",
        "     &END ALLEGRO\n",
        "    &END NONBONDED\n",
        "    &END FORCEFIELD\n",
        "    &POISSON\n",
        "      &EWALD\n",
        "        EWALD_TYPE none\n",
        "      &END EWALD\n",
        "    &END POISSON\n",
        "  &END MM\n",
        "  &SUBSYS\n",
        "    &CELL\n",
        "       ABC 24.68 25.648 48.0\n",
        "#      MULTIPLE_UNIT_CELL 2 2 2\n",
        "    &END CELL\n",
        "    &TOPOLOGY\n",
        "#     MULTIPLE_UNIT_CELL 2 2 2\n",
        "      COORD_FILE_NAME ./last_conf.xyz\n",
        "      COORD_FILE_FORMAT XYZ\n",
        "    &END TOPOLOGY\n",
        "  &END SUBSYS\n",
        "&END FORCE_EVAL\n",
        "&MOTION\n",
        "  &CONSTRAINT\n",
        "    &FIXED_ATOMS\n",
        "      LIST 1..480\n",
        "    &END\n",
        "  &END\n",
        "  &MD\n",
        "    ENSEMBLE NVT\n",
        "    STEPS 20000\n",
        "    TIMESTEP 0.5\n",
        "    TEMPERATURE 300\n",
        "    &THERMOSTAT\n",
        "      &CSVR\n",
        "        TIMECON 1\n",
        "      &END CSVR\n",
        "    &END\n",
        "  &END MD\n",
        "&END MOTION\n",
        "\"\"\"\n",
        "\n",
        "!mkdir cp2k_run\n",
        "with open(\"cp2k_run/Allegro_md.inp\", \"w\") as f:\n",
        "    f.write(cp2k_input_md)"
      ],
      "metadata": {
        "id": "2t20TtUtPL_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp2k_input_md = \"\"\"\n",
        "&GLOBAL\n",
        "  PROJECT water_bil_gra\n",
        "  RUN_TYPE MD\n",
        "&END GLOBAL\n",
        "&FORCE_EVAL\n",
        "  METHOD FIST\n",
        "  &MM\n",
        "    &FORCEFIELD\n",
        "     &NONBONDED\n",
        "     &NequIP\n",
        "        ATOMS C H O\n",
        "        PARM_FILE_NAME ./water-bil-gra-film-deploy-neq.pth\n",
        "        UNIT_COORDS angstrom\n",
        "        UNIT_ENERGY eV\n",
        "        UNIT_FORCES eV*Angstrom^-1\n",
        "     &END NequIP\n",
        "    &END NONBONDED\n",
        "    &END FORCEFIELD\n",
        "    &POISSON\n",
        "      &EWALD\n",
        "        EWALD_TYPE none\n",
        "      &END EWALD\n",
        "    &END POISSON\n",
        "  &END MM\n",
        "  &SUBSYS\n",
        "    &CELL\n",
        "       ABC 24.68 25.648 48.0\n",
        "#      MULTIPLE_UNIT_CELL 2 2 2\n",
        "    &END CELL\n",
        "    &TOPOLOGY\n",
        "#     MULTIPLE_UNIT_CELL 2 2 2\n",
        "      COORD_FILE_NAME ./last_conf.xyz\n",
        "      COORD_FILE_FORMAT XYZ\n",
        "    &END TOPOLOGY\n",
        "  &END SUBSYS\n",
        "&END FORCE_EVAL\n",
        "&MOTION\n",
        "#  &CONSTRAINT\n",
        "#    &FIXED_ATOMS\n",
        "#      LIST 1\n",
        "#    &END\n",
        "#  &END\n",
        "  &MD\n",
        "    ENSEMBLE NVT\n",
        "    STEPS 20000\n",
        "    TIMESTEP 0.5\n",
        "    TEMPERATURE 1\n",
        "    &THERMOSTAT\n",
        "      &CSVR\n",
        "        TIMECON 1\n",
        "      &END CSVR\n",
        "    &END\n",
        "  &END MD\n",
        "&END MOTION\n",
        "\"\"\"\n",
        "\n",
        "!mkdir cp2k_run\n",
        "with open(\"cp2k_run/NequIP_md.inp\", \"w\") as f:\n",
        "    f.write(cp2k_input_md)"
      ],
      "metadata": {
        "id": "k0wXvFA7yB1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plER-wpaFCCT"
      },
      "source": [
        "## Run 20,000 MD steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gurLjNK5upvq"
      },
      "outputs": [],
      "source": [
        "! cp *.pth last_conf.xyz cp2k_run/.\n",
        "if Model == \"Allegro\":\n",
        "  ! cd /content/cp2k_run && ../cp2k/exe/local_cuda/cp2k.ssmp -i Allegro_md.inp \n",
        "elif Model == \"NequIP\":\n",
        "  ! cd /content/cp2k_run && ../cp2k/exe/local_cuda/cp2k.ssmp -i NequIP_md.inp   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETDEkUHc39u4"
      },
      "source": [
        "### Visualize the trajectory with ase and ngl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGEqNBu2fmKQ"
      },
      "outputs": [],
      "source": [
        "from ase.visualize import view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYxvyvyK2ctM"
      },
      "outputs": [],
      "source": [
        "wat_traj = read(\"/content/cp2k_run/water_bil_gra-pos-1.xyz\", index=\"::\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G_nFd7dgWhc"
      },
      "outputs": [],
      "source": [
        "#water_cell_prod = np.array([9.85, 9.85, 9.85])\n",
        "water_cell_prod = np.array([24.68000, 25.64800,48.00000 ])\n",
        "\n",
        "for i in range(len(wat_traj)):\n",
        "    wat_traj[i].cell = water_cell_prod\n",
        "    wat_traj[i].pbc = np.array([True, True, True])\n",
        "    wat_traj[i].wrap()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "! pip install nglview\n",
        "!jupyter-nbextension enable nglview --py --sys-prefix\n",
        "import nglview as nv"
      ],
      "metadata": {
        "id": "Rcpzyc9WGqJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZLtkZ7bdszJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P5s2IbvfocL"
      },
      "outputs": [],
      "source": [
        "view(wat_traj, viewer=\"ngl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CMwyeFr4BAZ"
      },
      "source": [
        "### Calculate radial distribution function with MDAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV4F1NHf9WUo"
      },
      "outputs": [],
      "source": [
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import rdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtqT9im4AC15"
      },
      "outputs": [],
      "source": [
        "reader = mda.coordinates.XYZ.XYZReader(\"/content/cp2k_run/water-pos-1.xyz\")\n",
        "topology = mda.topology.XYZParser.XYZParser(\"/content/cp2k_run/water-pos-1.xyz\")\n",
        "\n",
        "u = mda.Universe(\"/content/cp2k_run/water-pos-1.xyz\")\n",
        "\n",
        "u.dimensions = [water_cell_prod[0], water_cell_prod[1], water_cell_prod[2], 90.0, 90.0, 90.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkoMQR8ZA0Sc"
      },
      "outputs": [],
      "source": [
        "O_at = u.select_atoms(\"name O\")\n",
        "H_at = u.select_atoms(\"name H\")\n",
        "\n",
        "Ordf = rdf.InterRDF(\n",
        "    O_at,\n",
        "    O_at,\n",
        "    nbins=100,  # default\n",
        "    range=(0.00001, water_cell_prod[0]/2),  # distance in angstroms\n",
        ")\n",
        "Ordf.run(start = 10000)\n",
        "\n",
        "OHrdf = rdf.InterRDF(\n",
        "    O_at,\n",
        "    H_at,\n",
        "    nbins=50,  # default\n",
        "    range=(0.00001, water_cell_prod[0]/2),  # distance in angstroms\n",
        ")\n",
        "OHrdf.run(start = 10000)\n",
        "\n",
        "HHrdf = rdf.InterRDF(\n",
        "    H_at,\n",
        "    H_at,\n",
        "    nbins=50,  # default\n",
        "    range=(0.00001, water_cell_prod[0]/2),  # distance in angstroms\n",
        ")\n",
        "HHrdf.run(start = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og07Lts5C1q_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(Ordf.bins, Ordf.rdf)\n",
        "plt.xlabel(\"Radius (angstrom)\")\n",
        "plt.ylabel(\"Radial distribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You can compare the O-O rdf above with the one obtained from a longer AIMD trajectory with the SCAN functional. The two rdfs are rather close to each other.\n",
        "\n",
        "https://www.pnas.org/doi/suppl/10.1073/pnas.2121641119/suppl_file/pnas.2121641119.sapp.pdf"
      ],
      "metadata": {
        "id": "ZIFyYvhZiPBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8dxSzw_L_3z"
      },
      "outputs": [],
      "source": [
        "plt.plot(OHrdf.bins, OHrdf.rdf)\n",
        "plt.xlabel(\"Radius (angstrom)\")\n",
        "plt.ylabel(\"Radial distribution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeT-d_udMAE0"
      },
      "outputs": [],
      "source": [
        "plt.plot(HHrdf.bins, HHrdf.rdf)\n",
        "plt.xlabel(\"Radius (angstrom)\")\n",
        "plt.ylabel(\"Radial distribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Total Energy and Temperature as a function of time after 5ps of equilibration to check the overall stability of the MD"
      ],
      "metadata": {
        "id": "h73fDoA3NWD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "ener_file = pd.read_csv(\"/content/cp2k_run/water-1.ener\", delim_whitespace=True, header = 0)\n",
        "\n",
        "ener_file = ener_file.drop(columns=\"#\").rename(columns = {\"Step\" : \"Time[fs]\", \"Nr.\": \"Kin.[a.u.]\",\n",
        "                                              \"Time[fs]\": \"Temp[K]\",\n",
        "                                              \"Kin.[a.u.]\": \"Pot [a.u.]\",\"Temp[K]\": \"Cons Qty[a.u.]\",\n",
        "                                              \"Pot.[a.u.]\": \"Used Time[s]\"\n",
        "                                              }).drop(columns=[\"Cons\",\"Qty[a.u.]\",\"UsedTime[s]\"])\n",
        "ener_file[10000:].plot(x=\"Time[fs]\",y=\"Pot [a.u.]\")\n",
        "ener_file[10000:].plot(x=\"Time[fs]\",y=\"Temp[K]\")\n"
      ],
      "metadata": {
        "id": "MHaa_9s41ZOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "interpreter": {
      "hash": "c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}