{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabriele16/abinitio-train/blob/main/abinitio-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpAi8g9XmUU"
      },
      "source": [
        "# Train an E(3)NN Interatomic Potential from ab initio data using NequIP or Allegro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyduv5hWIJYP"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/mir-group/allegro/blob/main/logo.png?raw=true\" width=\"30%\">\n",
        "<img src=\"https://github.com/mir-group/nequip/blob/main/logo.png?raw=true\" width=\"30%\">\n",
        "<center/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2shNisM863Wy"
      },
      "source": [
        "### Open in colab and change the runtime to use the GPU if you have not done so already\n",
        "\n",
        "### Install dependencies, these include pytorch, nequip, allegro and other common packages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HboutJMISv2t",
        "outputId": "d12fb24a-afb7-45b5-ab02-843a0a9667d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! rm -rf abinitio-train\n",
        "! git clone  https://github.com/gabriele16/abinitio-train.git\n",
        "! pip install -r requirements.txt\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "id": "hmmS-5nxkySM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n"
      ],
      "metadata": {
        "id": "egxttm2gUsw7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Pytorch and whether it works correctly on the GPU"
      ],
      "metadata": {
        "id": "MZNRk6K8NO-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZpOvFtImsy2",
        "outputId": "996bde70-1032-4bdf-a92a-1ec60e9c6c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************************\n",
            "torch version:  1.12.1+cu102\n",
            "*****************************\n",
            "cuda is available:  True\n",
            "*****************************\n",
            "cuda version:\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "*****************************\n",
            "check which GPU is being used (important only if attempting compilation of CP2K with CUDA):\n",
            "CP2K is optimized to run on the GPU with the folloging architectures: K20X, K40, K80, P100, V100, Mi50, Mi100, Mi250\n",
            "Fri May 12 13:36:16 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "*****************************\n",
            "check path where cuda is installed and adjust EXPORT PATH in the cp2k installation if necessary:\n",
            "*****************************\n",
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"*****************************\")\n",
        "print(\"torch version: \", torch.__version__)\n",
        "print(\"*****************************\")\n",
        "print(\"cuda is available: \", torch.cuda.is_available())\n",
        "print(\"*****************************\")\n",
        "print(\"cuda version:\")\n",
        "!nvcc --version\n",
        "print(\"*****************************\")\n",
        "print(\"check which GPU is being used:\")\n",
        "!nvidia-smi\n",
        "print(\"*****************************\")\n",
        "!which nvcc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrjXfT6L3S51"
      },
      "source": [
        "### Import PyTorch, numpy etc.\n",
        "### Set-up the directory containing the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J51CA0Bod1Jv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "from ase.io import read, write\n",
        "\n",
        "import warnings\n",
        "import os\n",
        "data_dir = \"/content/abinitio-train/datasets\"\n",
        "\n",
        "np.random.seed(0)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(0)\n",
        "else:\n",
        "  torch.manual_seed(0)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HDmxkn3z8_m"
      },
      "source": [
        "### Workflow:\n",
        "* Train: using a data set, train the neural network\n",
        "* Evaluate: evaluate the error on total energies and forces using unseen data\n",
        "* Deploy: convert the Python-based model into a stand-alone potential file for fast execution\n",
        "* Run: run Molecular Dynamics in CP2K "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aEgq6QYFIn"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KuOIippfVfd"
      },
      "source": [
        "Here, we extract some ab initio data set for bulk water or water in contact with graphene"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd abinitio-train/datasets/ && tar -xzvf wat_bil_gra_aimd.tar.gz\n",
        "! cd abinitio-train/datasets/ && tar -xzvf wat_gra_bil_film.tar.gz\n",
        "! cd abinitio-train/datasets/refined_water_gra && for i in *.tar.gz; do tar -xzvf $i; done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN4y9qQAljpO",
        "outputId": "dcf2dd0e-37b1-49fa-8aaf-50a159fd3c07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wat_bil_gra_aimd/\n",
            "wat_bil_gra_aimd/wat_pos_frc.extxyz\n",
            "wat_bil_gra_aimd/celldata.dat\n",
            "wat_gra_bil_film/\n",
            "wat_gra_bil_film/wat_pos_frc.extxyz\n",
            "wat_gra_bil_film/last_conf.xyz\n",
            "wat_gra_bil_film/celldata.dat\n",
            "wat_bil_gra_25/\n",
            "wat_bil_gra_25/wat_pos_frc.extxyz\n",
            "wat_bil_gra_25/celldata.dat\n",
            "wat_bil_gra_25/last_conf.xyz\n",
            "wat_bil_gra_30/\n",
            "wat_bil_gra_30/wat_pos_frc.extxyz\n",
            "wat_bil_gra_30/celldata.dat\n",
            "wat_bil_gra_30/last_conf.xyz\n",
            "wat_bil_gra_35/\n",
            "wat_bil_gra_35/wat_pos_frc.extxyz\n",
            "wat_bil_gra_35/celldata.dat\n",
            "wat_bil_gra_35/last_conf.xyz\n",
            "wat_bil_gra_40/\n",
            "wat_bil_gra_40/wat_pos_frc.extxyz\n",
            "wat_bil_gra_40/celldata.dat\n",
            "wat_bil_gra_40/last_conf.xyz\n",
            "wat_bil_gra_45/\n",
            "wat_bil_gra_45/wat_pos_frc.extxyz\n",
            "wat_bil_gra_45/celldata.dat\n",
            "wat_bil_gra_45/last_conf.xyz\n",
            "wat_bil_gra_60/\n",
            "wat_bil_gra_60/wat_pos_frc.extxyz\n",
            "wat_bil_gra_60/celldata.dat\n",
            "wat_bil_gra_60/last_conf.xyz\n",
            "wat_bil_gra_BULK/\n",
            "wat_bil_gra_BULK/wat_pos_frc.extxyz\n",
            "wat_bil_gra_BULK/celldata.dat\n",
            "wat_bil_gra_BULK/last_conf.xyz\n",
            "wat_bil_gra_FILM/\n",
            "wat_bil_gra_FILM/wat_pos_frc.extxyz\n",
            "wat_bil_gra_FILM/celldata.dat\n",
            "wat_bil_gra_FILM/last_conf.xyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd abinitio-train/datasets/ && tar -xzvf wat_film_gra.tar.gz"
      ],
      "metadata": {
        "id": "_8iEFXaFnND-",
        "outputId": "d5157825-8a19-4013-bd47-4b44fc0a3f2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wat_film_gra/./\n",
            "wat_film_gra/./wat_pos_frc.extxyz\n",
            "wat_film_gra/./celldata.dat\n",
            "wat_film_gra/./last_conf.xyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Below we explicitly write the input to train the model with NequIP/Allegro"
      ],
      "metadata": {
        "id": "PBPmf2CEgP1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allegro_input = \"\"\"\n",
        "# general\n",
        "root: results/water-gra-film\n",
        "run_name: water-gra-film\n",
        "seed: 42\n",
        "dataset_seed: 42\n",
        "append: true\n",
        "# To use float64 with cp2k, need to implement it, for now use float32\n",
        "default_dtype: float64\n",
        "\n",
        "# -- network --\n",
        "model_builders:\n",
        " - allegro.model.Allegro\n",
        " # the typical model builders from `nequip` can still be used:\n",
        " - PerSpeciesRescale\n",
        " - ForceOutput\n",
        " - RescaleEnergyEtc\n",
        "\n",
        "# cutoffs\n",
        "r_max: 5.0\n",
        "avg_num_neighbors: auto\n",
        "\n",
        "# radial basis\n",
        "# Try to use a small cutoff and a large polynomial cutoff p\n",
        "# use p=48 in the Li3PO4 case in the paper since the cutoff is quite small\n",
        "# see https://github.com/mir-group/allegro/discussions/20\n",
        "BesselBasis_trainable: true \n",
        "### Use normalize_basis: true, see https://github.com/mir-group/allegro/discussions/20\n",
        "normalize_basis: true\n",
        "PolynomialCutoff_p: 48   \n",
        "\n",
        "# symmetry\n",
        "l_max: 2\n",
        "parity: o3_full   \n",
        "\n",
        "# Allegro layers:\n",
        "num_layers: 2\n",
        "env_embed_multiplicity: 8\n",
        "embed_initial_edge: true\n",
        "\n",
        "two_body_latent_mlp_latent_dimensions: [32, 64, 128]\n",
        "two_body_latent_mlp_nonlinearity: silu\n",
        "two_body_latent_mlp_initialization: uniform\n",
        "\n",
        "latent_mlp_latent_dimensions: [128]\n",
        "latent_mlp_nonlinearity: silu\n",
        "latent_mlp_initialization: uniform\n",
        "latent_resnet: true\n",
        "\n",
        "env_embed_mlp_latent_dimensions: []\n",
        "env_embed_mlp_nonlinearity: null\n",
        "env_embed_mlp_initialization: uniform\n",
        "\n",
        "# - end allegro layers -\n",
        "\n",
        "# Final MLP to go from Allegro latent space to edge energies:\n",
        "edge_eng_mlp_latent_dimensions: [32]\n",
        "edge_eng_mlp_nonlinearity: null\n",
        "edge_eng_mlp_initialization: uniform\n",
        "\n",
        "include_keys:\n",
        "  - user_label\n",
        "key_mapping:\n",
        "  user_label: label0\n",
        "\n",
        "# -- data --\n",
        "dataset: ase                                                                   \n",
        "#dataset_file_name: /content/abinitio-train/datasets/wat_gra_bil_film/wat_pos_frc.extxyz                     # path to data set file\n",
        "#dataset_file_name: /content/abinitio-train/datasets/refined_water_gra/wat_bil_gra_FILM/wat_pos_frc.extxyz                     # path to data set file\n",
        "dataset_file_name: /content/abinitio-train/datasets/wat_film_gra/wat_pos_frc.extxyz                     # path to data set file\n",
        "\n",
        "ase_args:\n",
        "  format: extxyz\n",
        "\n",
        "# A mapping of chemical species to type indexes is necessary if the dataset is provided with atomic numbers instead of type indexes.\n",
        "#chemical_symbol_to_type:\n",
        "chemical_symbols:\n",
        "  - C\n",
        "  - H\n",
        "  - O\n",
        "\n",
        "# logging\n",
        "wandb: false\n",
        "#wandb_project: allegro-water-tutorial\n",
        "verbose: info\n",
        "log_batch_freq: 10\n",
        "\n",
        "# training\n",
        "n_train: 100\n",
        "n_val: 10\n",
        "batch_size: 1\n",
        "max_epochs: 200\n",
        "learning_rate: 0.002\n",
        "train_val_split: random\n",
        "shuffle: true\n",
        "metrics_key: validation_loss\n",
        "\n",
        "# use an exponential moving average of the weights\n",
        "use_ema: true\n",
        "ema_decay: 0.99\n",
        "ema_use_num_updates: true\n",
        "\n",
        "# loss function\n",
        "loss_coeffs:\n",
        "  forces: 1.\n",
        "  total_energy:\n",
        "    - 1.\n",
        "    - PerAtomMSELoss\n",
        "\n",
        "# optimizer\n",
        "optimizer_name: Adam\n",
        "optimizer_params:\n",
        "  amsgrad: false\n",
        "  betas: !!python/tuple\n",
        "  - 0.9\n",
        "  - 0.999\n",
        "  eps: 1.0e-08\n",
        "  weight_decay: 0.\n",
        "\n",
        "metrics_components:\n",
        "  - - forces                               # key \n",
        "    - mae                                  # \"rmse\" or \"mae\"\n",
        "  - - forces\n",
        "    - rmse\n",
        "  - - total_energy\n",
        "    - mae    \n",
        "  - - total_energy\n",
        "    - mae\n",
        "    - PerAtom: True                        # if true, energy is normalized by the number of atoms\n",
        "\n",
        "# lr scheduler, drop lr if no improvement for 50 epochs\n",
        "lr_scheduler_name: ReduceLROnPlateau\n",
        "lr_scheduler_patience: 25\n",
        "lr_scheduler_factor: 0.5\n",
        "\n",
        "early_stopping_lower_bounds:\n",
        "  LR: 1.0e-5\n",
        "\n",
        "early_stopping_patiences:\n",
        "  validation_loss: 25\n",
        "\"\"\"\n",
        "\n",
        "with open(\"allegro/configs/allegro-water-gra.yaml\", \"w\") as f:\n",
        "    f.write(allegro_input)"
      ],
      "metadata": {
        "id": "Meo8tyV0W_FS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nequip_input = \"\"\"\n",
        "\n",
        "# IMPORTANT: READ THIS\n",
        "\n",
        "# This is a full yaml file with all nequip options.\n",
        "# It is primarily intented to serve as documentation/reference for all options\n",
        "# For a simpler yaml file containing all necessary features to get you started, we strongly recommend to start with configs/example.yaml\n",
        "\n",
        "# Two folders will be used during the training: 'root'/process and 'root'/'run_name'\n",
        "# run_name contains logfiles and saved models\n",
        "# process contains processed data sets\n",
        "# if 'root'/'run_name' exists, 'root'/'run_name'_'year'-'month'-'day'-'hour'-'min'-'s' will be used instead.\n",
        "root: results/water-gra-film\n",
        "run_name: water-gra-film\n",
        "seed: 123 # model seed\n",
        "dataset_seed: 456 # data set seed\n",
        "append: true # set true if a restarted run should append to the previous log file\n",
        "default_dtype: float64 # type of float to use, e.g. float32 and float64\n",
        "allow_tf32: false # whether to use TensorFloat32 if it is available\n",
        "# device:  cuda                                                                   # which device to use. Default: automatically detected cuda or \"cpu\"\n",
        "\n",
        "# network\n",
        "r_max: 5.0 # cutoff radius in length units, here Angstrom, this is an important hyperparamter to scan\n",
        "num_layers: 3 # number of interaction blocks, we find 3-5 to work best\n",
        "\n",
        "l_max: 1 # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower\n",
        "parity: true # whether to include features with odd mirror parityy; often turning parity off gives equally good results but faster networks, so do consider this\n",
        "num_features: 32 # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower\n",
        "\n",
        "# alternatively, the irreps of the features in various parts of the network can be specified directly:\n",
        "# the following options use e3nn irreps notation\n",
        "# either these four options, or the above three options, should be provided--- they cannot be mixed.\n",
        "# chemical_embedding_irreps_out: 32x0e                                              # irreps for the chemical embedding of species\n",
        "# feature_irreps_hidden: 32x0o + 32x0e + 32x1o + 32x1e                              # irreps used for hidden features, here we go up to lmax=1, with even and odd parities; for more accurate but slower networks, use l=2 or higher, smaller number of features is faster\n",
        "# irreps_edge_sh: 0e + 1o                                                           # irreps of the spherical harmonics used for edges. If a single integer, indicates the full SH up to L_max=that_integer\n",
        "# conv_to_output_hidden_irreps_out: 16x0e                                           # irreps used in hidden layer of output block\n",
        "\n",
        "nonlinearity_type: gate # may be 'gate' or 'norm', 'gate' is recommended\n",
        "resnet:\n",
        "  false # set true to make interaction block a resnet-style update\n",
        "  # the resnet update will only be applied when the input and output irreps of the layer are the same\n",
        "\n",
        "# scalar nonlinearities to use — available options are silu, ssp (shifted softplus), tanh, and abs.\n",
        "# Different nonlinearities are specified for e (even) and o (odd) parity;\n",
        "# note that only tanh and abs are correct for o (odd parity).\n",
        "# silu typically works best for even\n",
        "nonlinearity_scalars:\n",
        "  e: silu\n",
        "  o: tanh\n",
        "\n",
        "nonlinearity_gates:\n",
        "  e: silu\n",
        "  o: tanh\n",
        "\n",
        "# radial network basis\n",
        "num_basis: 8 # number of basis functions used in the radial basis, 8 usually works best\n",
        "BesselBasis_trainable: true # set true to train the bessel weights\n",
        "PolynomialCutoff_p: 48 # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance\n",
        "\n",
        "# radial network\n",
        "invariant_layers: 2 # number of radial layers, usually 1-3 works best, smaller is faster\n",
        "invariant_neurons: 64 # number of hidden neurons in radial function, smaller is faster\n",
        "avg_num_neighbors: auto # number of neighbors to divide by, null => no normalization, auto computes it based on dataset\n",
        "use_sc: true # use self-connection or not, usually gives big improvement\n",
        "\n",
        "# to specify different parameters for each convolutional layer, try examples below\n",
        "# layer1_use_sc: true                                                             # use \"layer{i}_\" prefix to specify parameters for only one of the layer,\n",
        "# priority for different definitions:\n",
        "#   invariant_neurons < InteractionBlock_invariant_neurons < layer{i}_invariant_neurons\n",
        "\n",
        "# data set\n",
        "# there are two options to specify a dataset, npz or ase\n",
        "# npz works with npz files, ase can ready any format that ase.io.read can read\n",
        "# in most cases working with the ase option and an extxyz file is by far the simplest way to do it and we strongly recommend using this\n",
        "# simply provide a single extxyz file that contains the structures together with energies and forces (generated with ase.io.write(atoms, format='extxyz', append=True))\n",
        "\n",
        "include_keys:\n",
        "  - user_label\n",
        "key_mapping:\n",
        "  user_label: label0\n",
        "\n",
        "# alternatively, you can read directly from a VASP OUTCAR file (this will only read that single OUTCAR)\n",
        "# # for VASP OUTCAR, the yaml input should be\n",
        "# dataset: ase\n",
        "# dataset_file_name: OUTCAR\n",
        "# ase_args:\n",
        "#   format: vasp-out\n",
        "# important VASP note: the ase vasp parser stores the potential energy to \"free_energy\" instead of \"energy\".\n",
        "# Here, the key_mapping maps the external name (key) to the NequIP default name (value)\n",
        "# key_mapping:\n",
        "#   free_energy: total_energy\n",
        "\n",
        "# npz example\n",
        "# the keys used need to be stated at least once in key_mapping, npz_fixed_field_keys or include_keys\n",
        "# key_mapping is used to map the key in the npz file to the NequIP default values (see data/_key.py)\n",
        "# all arrays are expected to have the shape of (nframe, natom, ?) except the fixed fields\n",
        "# note that if your data set uses pbc, you need to also pass an array that maps to the nequip \"pbc\" key\n",
        "# dataset: npz                                                                       # type of data set, can be npz or ase\n",
        "# dataset_url: http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip           # url to download the npz. optional\n",
        "# dataset_file_name: ./benchmark_data/toluene_ccsd_t-train.npz                       # path to data set file\n",
        "# key_mapping:\n",
        "#   z: atomic_numbers                                                                # atomic species, integers\n",
        "#   E: total_energy                                                                  # total potential eneriges to train to\n",
        "#   F: forces                                                                        # atomic forces to train to\n",
        "#   R: pos                                                                           # raw atomic positions\n",
        "# npz_fixed_field_keys:                                                              # fields that are repeated across different examples\n",
        "#   - atomic_numbers\n",
        "\n",
        "# A list of chemical species found in the data. The NequIP atom types will be named after the chemical symbols and ordered by atomic number in ascending order.\n",
        "# (In this case, NequIP's internal atom type 0 will be named H and type 1 will be named C.)\n",
        "# Atoms in the input will be assigned NequIP atom types according to their atomic numbers.\n",
        "chemical_symbols:\n",
        "  - C\n",
        "  - H\n",
        "  - O\n",
        "\n",
        "# Alternatively, you may explicitly specify which chemical species in the input will map to NequIP atom type 0, which to atom type 1, and so on.\n",
        "# Other than providing an explicit order for the NequIP atom types, this option behaves the same as `chemical_symbols`\n",
        "#chemical_symbol_to_type:\n",
        "#  C: 0\n",
        "#  H: 1\n",
        "#  O: 2\n",
        "\n",
        "# Alternatively, if the dataset has type indices, you may give the names for the types in order:\n",
        "# (this also sets the number of types)\n",
        "# type_names:\n",
        "#   - my_type\n",
        "#   - atom\n",
        "#   - thing\n",
        "\n",
        "# As an alternative option to npz, you can also pass data ase ASE Atoms-objects\n",
        "# This can often be easier to work with, simply make sure the ASE Atoms object\n",
        "# has a calculator for which atoms.get_potential_energy() and atoms.get_forces() are defined\n",
        "dataset: ase\n",
        "#dataset_file_name: ./abinitio-train/datasets/wat_gra_bil_film/wat_pos_frc.extxyz # need to be a format accepted by ase.io.read\n",
        "#dataset_file_name: /content/abinitio-train/datasets/refined_water_gra/wat_bil_gra_FILM/wat_pos_frc.extxyz                     # path to data set file\n",
        "dataset_file_name: /content/abinitio-train/datasets/wat_film_gra/wat_pos_frc.extxyz                     # path to data set file\n",
        "\n",
        "ase_args: # any arguments needed by ase.io.read\n",
        "  format: extxyz\n",
        "\n",
        "# If you want to use a different dataset for validation, you can specify\n",
        "# the same types of options using a `validation_` prefix:\n",
        "# validation_dataset: ase\n",
        "# validation_dataset_file_name: xxx.xyz                                            # need to be a format accepted by ase.io.read\n",
        "\n",
        "# logging\n",
        "#wandb: true # we recommend using wandb for logging\n",
        "#wandb_project: water-example # project name used in wandb\n",
        "#wandb_watch: false\n",
        "\n",
        "# see https://docs.wandb.ai/ref/python/watch\n",
        "# wandb_watch_kwargs:\n",
        "#   log: all\n",
        "#   log_freq: 1\n",
        "#   log_graph: true\n",
        "\n",
        "verbose: info # the same as python logging, e.g. warning, info, debug, error. case insensitive\n",
        "log_batch_freq: 1 # batch frequency, how often to print training errors withinin the same epoch\n",
        "log_epoch_freq: 1 # epoch frequency, how often to print\n",
        "save_checkpoint_freq: -1 # frequency to save the intermediate checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
        "save_ema_checkpoint_freq: -1 # frequency to save the intermediate ema checkpoint. no saving of intermediate checkpoints when the value is not positive.\n",
        "\n",
        "# training\n",
        "n_train: 100 # number of training data\n",
        "n_val: 10 # number of validation data\n",
        "learning_rate: 0.005 # learning rate, we found values between 0.01 and 0.005 to work best - this is often one of the most important hyperparameters to tune\n",
        "batch_size: 1 # batch size, we found it important to keep this small for most applications including forces (1-5); for energy-only training, higher batch sizes work better\n",
        "validation_batch_size: 1 # batch size for evaluating the model during validation. This does not affect the training results, but using the highest value possible (<=n_val) without running out of memory will speed up your training.\n",
        "max_epochs: 200 # stop training after _ number of epochs, we set a very large number here, it won't take this long in practice and we will use early stopping instead\n",
        "train_val_split: random # can be random or sequential. if sequential, first n_train elements are training, next n_val are val, else random, usually random is the right choice\n",
        "shuffle: true # If true, the data loader will shuffle the data, usually a good idea\n",
        "metrics_key: validation_loss # metrics used for scheduling and saving best model. Options: `set`_`quantity`, set can be either \"train\" or \"validation, \"quantity\" can be loss or anything that appears in the validation batch step header, such as f_mae, f_rmse, e_mae, e_rmse\n",
        "use_ema: true # if true, use exponential moving average on weights for val/test, usually helps a lot with training, in particular for energy errors\n",
        "ema_decay: 0.99 # ema weight, typically set to 0.99 or 0.999\n",
        "ema_use_num_updates: true # whether to use number of updates when computing averages\n",
        "report_init_validation: true # if True, report the validation error for just initialized model\n",
        "\n",
        "# early stopping based on metrics values.\n",
        "# LR, wall and any keys printed in the log file can be used.\n",
        "# The key can start with Training or validation. If not defined, the validation value will be used.\n",
        "early_stopping_patiences: # stop early if a metric value stopped decreasing for n epochs\n",
        "  validation_loss: 50\n",
        "\n",
        "early_stopping_delta: # If delta is defined, a decrease smaller than delta will not be considered as a decrease\n",
        "  validation_loss: 0.005\n",
        "\n",
        "early_stopping_cumulative_delta: false # If True, the minimum value recorded will not be updated when the decrease is smaller than delta\n",
        "\n",
        "early_stopping_lower_bounds: # stop early if a metric value is lower than the bound\n",
        "  LR: 1.0e-5\n",
        "\n",
        "early_stopping_upper_bounds: # stop early if a metric value is higher than the bound\n",
        "  cumulative_wall: 1.0e+100\n",
        "\n",
        "# loss function\n",
        "loss_coeffs: # different weights to use in a weighted loss functions\n",
        "  forces: 1 # if using PerAtomMSELoss, a default weight of 1:1 on each should work well\n",
        "  total_energy:\n",
        "    - 1\n",
        "    - PerAtomMSELoss\n",
        "\n",
        "# # default loss function is MSELoss, the name has to be exactly the same as those in torch.nn.\n",
        "# the only supprted targets are forces and total_energy\n",
        "\n",
        "# here are some example of more ways to declare different types of loss functions, depending on your application:\n",
        "# loss_coeffs:\n",
        "#   total_energy: MSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 3.0\n",
        "#   - MSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 1.0\n",
        "#   - PerAtomMSELoss\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   forces:\n",
        "#   - 1.0\n",
        "#   - PerSpeciesL1Loss\n",
        "#\n",
        "# loss_coeffs: total_energy\n",
        "#\n",
        "# loss_coeffs:\n",
        "#   total_energy:\n",
        "#   - 3.0\n",
        "#   - L1Loss\n",
        "#   forces: 1.0\n",
        "\n",
        "# output metrics\n",
        "metrics_components:\n",
        "  - - forces # key\n",
        "    - mae # \"rmse\" or \"mae\"\n",
        "  - - forces\n",
        "    - rmse\n",
        "  - - forces\n",
        "    - mae\n",
        "    - PerSpecies: True # if true, per species contribution is counted separately\n",
        "      report_per_component: False # if true, statistics on each component (i.e. fx, fy, fz) will be counted separately\n",
        "  - - forces\n",
        "    - rmse\n",
        "    - PerSpecies: True\n",
        "      report_per_component: False\n",
        "  - - total_energy\n",
        "    - mae\n",
        "  - - total_energy\n",
        "    - mae\n",
        "    - PerAtom: True # if true, energy is normalized by the number of atoms\n",
        "\n",
        "# optimizer, may be any optimizer defined in torch.optim\n",
        "# the name `optimizer_name`is case sensitive\n",
        "optimizer_name: Adam # default optimizer is Adam\n",
        "optimizer_amsgrad: false\n",
        "optimizer_betas: !!python/tuple\n",
        "  - 0.9\n",
        "  - 0.999\n",
        "optimizer_eps: 1.0e-08\n",
        "optimizer_weight_decay: 0\n",
        "\n",
        "# gradient clipping using torch.nn.utils.clip_grad_norm_\n",
        "# see https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\n",
        "# setting to inf or null disables it\n",
        "max_gradient_norm: null\n",
        "\n",
        "# lr scheduler, currently only supports the two options listed below, if you need more please file an issue\n",
        "# first: on-plateau, reduce lr by factory of lr_scheduler_factor if metrics_key hasn't improved for lr_scheduler_patience epoch\n",
        "lr_scheduler_name: ReduceLROnPlateau\n",
        "lr_scheduler_patience: 100\n",
        "lr_scheduler_factor: 0.5\n",
        "\n",
        "# second, cosine annealing with warm restart\n",
        "# lr_scheduler_name: CosineAnnealingWarmRestarts\n",
        "# lr_scheduler_T_0: 10000\n",
        "# lr_scheduler_T_mult: 2\n",
        "# lr_scheduler_eta_min: 0\n",
        "# lr_scheduler_last_epoch: -1\n",
        "\n",
        "# we provide a series of options to shift and scale the data\n",
        "# these are for advanced use and usually the defaults work very well\n",
        "# the default is to scale the energies and forces by scaling them by the force standard deviation and to shift the energy by its mean\n",
        "# in certain cases, it can be useful to have a trainable shift/scale and to also have species-dependent shifts/scales for each atom\n",
        "\n",
        "per_species_rescale_scales_trainable: false\n",
        "# whether the scales are trainable. Defaults to False. Optional\n",
        "per_species_rescale_shifts_trainable: false\n",
        "# whether the shifts are trainable. Defaults to False. Optional\n",
        "per_species_rescale_shifts: dataset_per_atom_total_energy_mean\n",
        "# initial atomic energy shift for each species. default to the mean of per atom energy. Optional\n",
        "# the value can be a constant float value, an array for each species, or a string\n",
        "# string option include:\n",
        "# *  \"dataset_per_atom_total_energy_mean\", which computes the per atom average\n",
        "# *  \"dataset_per_species_total_energy_mean\", which automatically compute the per atom energy mean using a GP model\n",
        "per_species_rescale_scales: dataset_forces_rms\n",
        "# initial atomic energy scale for each species. Optional.\n",
        "# the value can be a constant float value, an array for each species, or a string\n",
        "# string option include:\n",
        "# *  \"dataset_per_atom_total_energy_std\", which computes the per atom energy std\n",
        "# *  \"dataset_per_species_total_energy_std\", which uses the GP model uncertainty\n",
        "# *  \"dataset_per_species_forces_rms\", which compute the force rms for each species\n",
        "# If not provided, defaults to dataset_per_species_force_rms or dataset_per_atom_total_energy_std, depending on whether forces are being trained.\n",
        "# per_species_rescale_kwargs:\n",
        "#   total_energy:\n",
        "#     alpha: 0.1\n",
        "#     max_iteration: 20\n",
        "#     stride: 100\n",
        "# keywords for GP decomposition of per specie energy. Optional. Defaults to 0.1\n",
        "# per_species_rescale_arguments_in_dataset_units: True\n",
        "# if explicit numbers are given for the shifts/scales, this parameter must specify whether the given numbers are unitless shifts/scales or are in the units of the dataset. If ``True``, any global rescalings will correctly be applied to the per-species values.\n",
        "\n",
        "# global energy shift and scale\n",
        "# When \"dataset_total_energy_mean\", the mean energy of the dataset. When None, disables the global shift. When a number, used directly.\n",
        "# Warning: if this value is not None, the model is no longer size extensive\n",
        "global_rescale_shift: null\n",
        "\n",
        "# global energy scale. When \"dataset_force_rms\", the RMS of force components in the dataset. When \"dataset_total_energy_std\", the stdev of energies in the dataset. When null, disables the global scale. When a number, used directly.\n",
        "# If not provided, defaults to either dataset_force_rms or dataset_total_energy_std, depending on whether forces are being trained.\n",
        "global_rescale_scale: dataset_forces_rms\n",
        "\n",
        "# whether the shift of the final global energy rescaling should be trainable\n",
        "global_rescale_shift_trainable: false\n",
        "\n",
        "# whether the scale of the final global energy rescaling should be trainable\n",
        "global_rescale_scale_trainable: false\n",
        "# # full block needed for per specie rescale\n",
        "# global_rescale_shift: null\n",
        "# global_rescale_shift_trainable: false\n",
        "# global_rescale_scale: dataset_forces_rms\n",
        "# global_rescale_scale_trainable: false\n",
        "# per_species_rescale_trainable: true\n",
        "# per_species_rescale_shifts: dataset_per_atom_total_energy_mean\n",
        "# per_species_rescale_scales: dataset_per_atom_total_energy_std\n",
        "\n",
        "# # full block needed for global rescale\n",
        "# global_rescale_shift: dataset_total_energy_mean\n",
        "# global_rescale_shift_trainable: false\n",
        "# global_rescale_scale: dataset_forces_rms\n",
        "# global_rescale_scale_trainable: false\n",
        "# per_species_rescale_trainable: false\n",
        "# per_species_rescale_shifts: null\n",
        "# per_species_rescale_scales: null\n",
        "\n",
        "# Options for e3nn's set_optimization_defaults. A dict:\n",
        "# e3nn_optimization_defaults:\n",
        "#   explicit_backward: True\n",
        "\"\"\"\n",
        "\n",
        "!mkdir nequip_train\n",
        "with open(\"nequip_train/water-gra.yaml\", \"w\") as f:\n",
        "    f.write(nequip_input)"
      ],
      "metadata": {
        "id": "TQYNvq6YO0EH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with Allegro or NequIP"
      ],
      "metadata": {
        "id": "33biTiWVq7Zb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukSnt_QD5avu",
        "outputId": "feddf8a5-f677-41c6-d103-782a1d0cb38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "Processing dataset...\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./results\n",
        "\n",
        "Model='Allegro'\n",
        "\n",
        "if Model=='Allegro':\n",
        "  !nequip-train allegro/configs/allegro-water-gra.yaml  \n",
        "elif Model=='NequIP':\n",
        "  !nequip-train nequip_train/water-gra.yaml --equivariance-test\n",
        "else:\n",
        "  print(\"Model has to be either Allegro or NequIP\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nequip-train --help"
      ],
      "metadata": {
        "id": "RnqHooUBoIjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the test error\n",
        "##### We get rather small errors in the forces of ~50 meV/A"
      ],
      "metadata": {
        "id": "Wev2u1UyrVCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nequip-evaluate --train-dir results/water-gra-film/water-gra-film --batch-size 1"
      ],
      "metadata": {
        "id": "KlEaWYSgrXtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJitSZgLYNNF"
      },
      "source": [
        "### Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoeGtlA02KQ"
      },
      "source": [
        "We now convert the model to a potential file. This makes it independent of NequIP and we can load it in CP2K to run MD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3NJJgtDIDNc"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import subprocess\n",
        "\n",
        "# datetime object containing current date and time\n",
        "depl_time = datetime.now().strftime(\"%d%m%Y-%H%M\")\n",
        "\n",
        "if Model == \"Allegro\":\n",
        "  !nequip-deploy build --train-dir results/water-gra-film/water-gra-film water-gra-film-deploy-alle.pth\n",
        "  cmd = [\"cp\", \"water-gra-film-deploy-alle.pth\", \"water-gra-film-deploy-alle-\"+depl_time+\".pth\"] \n",
        "elif Model == \"NequIP\":\n",
        "   !nequip-deploy build --train-dir results/water-gra-film/water-gra-film water-gra-film-deploy-neq.pth \n",
        "   cmd = [\"cp\", \"water-gra-film-deploy-neq.pth\", \"water-gra-film-deploy-neq-\"+depl_time+\".pth\"] \n",
        "\n",
        "subprocess.Popen(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_is_pretrained = False\n",
        "\n",
        "if model_is_pretrained == False:\n",
        "  ! cp *2023*.pth /content/drive/MyDrive/models_and_datasets/models/.\n",
        "else:\n",
        "  ! cp /content/drive/MyDrive/models_and_datasets/models/*.pth ."
      ],
      "metadata": {
        "id": "9uFoAb6AHD8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "interpreter": {
      "hash": "c9be9acec9edbd902b751bf46a8fbd7b71bbc5f0438c72d3ebaee4bffeb5e5e4"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}